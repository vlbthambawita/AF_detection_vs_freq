{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59395c5f",
   "metadata": {},
   "source": [
    "# ECG Arrhythmia 1D-CNN (binary AF detector)\n",
    "\n",
    "This notebook loads the ECG dataset in `ecg_arrhythmia_dataset_CSV`, prepares a binary label for Atrial Fibrillation (AF / AFIB), builds a small 1D convolutional neural network (Conv1D) that accepts 12-lead ECG signals, and trains it. The notebook uses a configurable `max_samples` and `downsample_factor` so you can run quickly on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f132d7",
   "metadata": {},
   "source": [
    "**PyTorch version**\n",
    "\n",
    "The cells below provide an equivalent pipeline implemented with PyTorch: data loading, a `torch.utils.data.DataLoader`, a small Conv1D model, training loop, evaluation, and model saving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273af566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pandas torch torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sklearn\n",
    "print('torch', torch.__version__)\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('scikit-learn', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a10e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('torch', torch.__version__)\n",
    "\n",
    "# Parameters (adjust before running)\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT / 'ecg_arrhythmia_dataset_CSV'\n",
    "WFDB_DIR = DATA_DIR / 'WFDBRecords'\n",
    "MAPPING_CSV = DATA_DIR / 'file_mapping.csv'\n",
    "# Quick test: use one quarter of the dataset. Set to None to use entire mapping.\n",
    "MAX_SAMPLES = 'third'  # 'quarter' => int(len(mapping)/4)\n",
    "DOWNSAMPLE = None  # downsampling disabled. Set to int > 1 to enable\n",
    "EPOCHS = 6          # run X amount of epochs for quick test\n",
    "BATCH_SIZE = 8      # smaller batch for testing\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMNS = ['AF', 'AFIB']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0966730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a streaming (on-disk) PyTorch Dataset so we don't load all records into memory\n",
    "mapping = pd.read_csv(MAPPING_CSV)\n",
    "\n",
    "# Robustly determine AF label column and create `af_label`\n",
    "if all(col in mapping.columns for col in TARGET_COLUMNS):\n",
    "    mapping['af_label'] = (mapping[TARGET_COLUMNS].sum(axis=1) > 0).astype(int)\n",
    "else:\n",
    "    # try a list of common candidate column names\n",
    "    candidates = ['af_label','AFIB','afib','is_af','is_afib','label','arrhythmia']\n",
    "    found = [c for c in candidates if c in mapping.columns]\n",
    "    if found:\n",
    "        # if the found column is numeric/binary, use it; otherwise try string matching\n",
    "        col = found[0]\n",
    "        if np.issubdtype(mapping[col].dtype, np.number):\n",
    "            mapping['af_label'] = (mapping[col] > 0).astype(int)\n",
    "        else:\n",
    "            mapping['af_label'] = mapping[col].astype(str).str.contains('AF', case=False, na=False).astype(int)\n",
    "    elif 'diagnosis' in mapping.columns:\n",
    "        mapping['af_label'] = mapping['diagnosis'].astype(str).str.contains('AF', case=False, na=False).astype(int)\n",
    "    else:\n",
    "        mapping['af_label'] = 0\n",
    "        print(\"Warning: couldn't find AF label column; setting all labels to 0 (no AF samples)\")\n",
    "\n",
    "# If MAX_SAMPLES was set to an integer computed earlier, keep it; if it's a sentinel string like 'quarter' or 'third', handle it\n",
    "if isinstance(MAX_SAMPLES, str):\n",
    "    if MAX_SAMPLES == 'quarter':\n",
    "        MAX_SAMPLES = int(len(mapping) / 4)\n",
    "    elif MAX_SAMPLES == 'third':\n",
    "        MAX_SAMPLES = int(len(mapping) / 3)\n",
    "\n",
    "if MAX_SAMPLES is not None and isinstance(MAX_SAMPLES, (int, np.integer)):\n",
    "    mapping = mapping.iloc[:int(MAX_SAMPLES)].reset_index(drop=True)\n",
    "\n",
    "# Shuffle / split\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "all_idx = np.arange(len(mapping))\n",
    "rng.shuffle(all_idx)\n",
    "\n",
    "n = len(all_idx)\n",
    "n_train = int(n * 0.8)\n",
    "n_val = int(n * 0.1)\n",
    "\n",
    "idx_train = all_idx[:n_train]\n",
    "idx_val = all_idx[n_train:n_train + n_val]\n",
    "idx_test = all_idx[n_train + n_val:]\n",
    "\n",
    "print(f\"train samples {len(idx_train)} val samples {len(idx_val)} test samples {len(idx_test)}\")\n",
    "print(\"Example record path: (columns 'subdir'/'filename' not present in mapping)\")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = ECGOnDiskDataset(mapping, WFDB_DIR, idx_train, downsample=DOWNSAMPLE, target_col='af_label')\n",
    "val_ds = ECGOnDiskDataset(mapping, WFDB_DIR, idx_val, downsample=DOWNSAMPLE, target_col='af_label')\n",
    "test_ds = ECGOnDiskDataset(mapping, WFDB_DIR, idx_test, downsample=DOWNSAMPLE, target_col='af_label')\n",
    "\n",
    "# DataLoaders (num_workers=0 for Windows)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run test: fetch a single batch from the `train_loader` to verify streaming loader and shapes\n",
    "#try:\n",
    "#    xb, yb = next(iter(train_loader))\n",
    "#    print('Dry-run OK — batch shapes: x=', xb.shape, ' y=', yb.shape)\n",
    "#    # show dtype and device info\n",
    "#    print('x dtype:', xb.dtype, ' y dtype:', yb.dtype)\n",
    "#    print('Sample label counts:', int((yb>0.5).sum()), 'positive out of', yb.size(0))\n",
    "#except Exception as e:\n",
    "#    print('Dry-run failed:', repr(e))\n",
    "#    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch 1D-CNN model and instantiate it before training\n",
    "class ECG1DCNN(nn.Module):\n",
    "    def __init__(self, in_channels=12, num_classes=1, channels=[32,64,128], kernel_size=7):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_channels\n",
    "        for ch in channels:\n",
    "            layers += [\n",
    "                nn.Conv1d(prev, ch, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.BatchNorm1d(ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(2)\n",
    "            ]\n",
    "            prev = ch\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(prev, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, timesteps)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)  # (batch, channels, 1)\n",
    "        x = self.head(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# Instantiate model and move to device\n",
    "model = ECG1DCNN(in_channels=12, num_classes=1).to(DEVICE)\n",
    "print('Model instantiated. Parameters:', sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model and run a single forward pass (no optimizer step)\n",
    "import traceback\n",
    "try:\n",
    "    # Re-create the model (fresh weights)\n",
    "    model = ECG1DCNN(in_channels=12, num_classes=1).to(DEVICE)\n",
    "    print('Model re-initialized. Parameters:', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "    # Fetch a single batch\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    print('Batch shapes:', xb.shape, yb.shape)\n",
    "\n",
    "    xb_dev = xb.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(xb_dev)\n",
    "\n",
    "    print('logits NaN:', torch.isnan(logits).any().item(), 'Inf:', torch.isinf(logits).any().item())\n",
    "    # print stats safely using .detach()\n",
    "    if not torch.isnan(logits).any().item() and not torch.isinf(logits).any().item():\n",
    "        l = logits.detach().cpu()\n",
    "        print('logits min/max/mean/std:', float(l.min()), float(l.max()), float(l.mean()), float(l.std()))\n",
    "        print('sample logits (first 8):', l[:8].numpy())\n",
    "    else:\n",
    "        print('Logits contain NaN/Inf — forward pass unstable on fresh model')\n",
    "except Exception as e:\n",
    "    print('Re-init forward pass failed:', e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation trace: run input through each layer and print stats to find NaNs\n",
    "import torch\n",
    "import math\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "xb = xb.to(DEVICE)\n",
    "print('Input batch shape:', xb.shape)\n",
    "\n",
    "x = xb\n",
    "print('Input NaN/Inf:', torch.isnan(x).any().item(), torch.isinf(x).any().item())\n",
    "\n",
    "# Trace through feature layers\n",
    "for i, layer in enumerate(model.features):\n",
    "    x = layer(x)\n",
    "    has_nan = torch.isnan(x).any().item()\n",
    "    has_inf = torch.isinf(x).any().item()\n",
    "    if not has_nan and not has_inf:\n",
    "        vmin = float(x.min()); vmax = float(x.max()); vmean = float(x.mean()); vstd = float(x.std())\n",
    "    else:\n",
    "        vmin = vmax = vmean = vstd = float('nan')\n",
    "    print(f'features[{i}] {layer.__class__.__name__} -> shape={x.shape} NaN={has_nan} Inf={has_inf} min={vmin} max={vmax} mean={vmean} std={vstd}')\n",
    "    if has_nan or has_inf:\n",
    "        print('NaN/Inf detected at features index', i, 'layer:', layer)\n",
    "        break\n",
    "\n",
    "# If features completed without NaN, check pool and head\n",
    "if not (torch.isnan(x).any() or torch.isinf(x).any()):\n",
    "    x = model.pool(x)\n",
    "    print('After pool shape:', x.shape, 'NaN:', torch.isnan(x).any().item())\n",
    "    # pass through head sequentially\n",
    "    for j, layer in enumerate(model.head):\n",
    "        x = layer(x)\n",
    "        has_nan = torch.isnan(x).any().item()\n",
    "        has_inf = torch.isinf(x).any().item()\n",
    "        print(f'head[{j}] {layer.__class__.__name__} -> shape={x.shape} NaN={has_nan} Inf={has_inf}')\n",
    "        if has_nan or has_inf:\n",
    "            print('NaN/Inf detected at head index', j, 'layer:', layer)\n",
    "            break\n",
    "else:\n",
    "    print('Stopping trace because NaN/Inf already present in features output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# training loop with timing\n",
    "start_time = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds.extend(probs.tolist())\n",
    "            targets.extend(yb.cpu().numpy().tolist())\n",
    "    val_loss /= len(val_loader.dataset) if len(val_loader.dataset)>0 else 1.0\n",
    "    try:\n",
    "        val_auc = roc_auc_score(targets, preds)\n",
    "    except Exception:\n",
    "        val_auc = float('nan')\n",
    "    print(f'Epoch {epoch}/{EPOCHS}  train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_auc={val_auc:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "TRAIN_RUNTIME_SECONDS = int(end_time - start_time)\n",
    "print('Training runtime (s):', TRAIN_RUNTIME_SECONDS)\n",
    "\n",
    "# save final model\n",
    "out_dir = Path('models')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), out_dir / 'pytorch_ecg_1dcnn.pth')\n",
    "print('Saved PyTorch model to', out_dir / 'pytorch_ecg_1dcnn.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Metrics Summary ===\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Ensure test set exists\n",
    "if len(test_ds) == 0:\n",
    "    print('No separate test set available (test set length = 0). Using validation set as test set for summary.')\n",
    "    eval_loader = val_loader\n",
    "    eval_size = len(val_ds)\n",
    "else:\n",
    "    eval_loader = test_loader\n",
    "    eval_size = len(test_ds)\n",
    "\n",
    "# Evaluate on chosen test loader\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_probs = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in eval_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        test_loss += loss.item() * xb.size(0)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_targets.extend(yb.cpu().numpy().tolist())\n",
    "\n",
    "test_loss = test_loss / eval_size if eval_size>0 else float('nan')\n",
    "# predictions\n",
    "y_pred = [1 if p>=0.5 else 0 for p in all_probs]\n",
    "\n",
    "# metrics\n",
    "acc = accuracy_score(all_targets, y_pred) if eval_size>0 else float('nan')\n",
    "f1 = f1_score(all_targets, y_pred, zero_division=0) if eval_size>0 else float('nan')\n",
    "precision, recall, f1_per_label, _ = precision_recall_fscore_support(all_targets, y_pred, zero_division=0)\n",
    "clf_report = classification_report(all_targets, y_pred, target_names=['Normal','AFIB'], zero_division=0)\n",
    "cm = confusion_matrix(all_targets, y_pred)\n",
    "\n",
    "# sampling frequency: take mode of selected records' sampling_frequency (original), then divide by downsample\n",
    "try:\n",
    "    sel_sampling = mapping.loc[sel_idx, 'sampling_frequency']\n",
    "    sampling_freq = float(sel_sampling.mode().iloc[0]) / (DOWNSAMPLE if DOWNSAMPLE and DOWNSAMPLE>0 else 1)\n",
    "except Exception:\n",
    "    sampling_freq = float('nan')\n",
    "\n",
    "# runtime formatting\n",
    "try:\n",
    "    seconds = TRAIN_RUNTIME_SECONDS\n",
    "    runtime_str = str(datetime.timedelta(seconds=seconds))\n",
    "except Exception:\n",
    "    runtime_str = 'N/A'\n",
    "\n",
    "# dataset sizes\n",
    "train_n = len(train_ds)\n",
    "val_n = len(val_ds)\n",
    "test_n = len(test_ds)\n",
    "\n",
    "# Print summary\n",
    "print('=== Test Metrics Summary ===')\n",
    "print(f'  Test Loss:     {test_loss:.4f}')\n",
    "print(f'  Test F1-score: {f1:.4f}')\n",
    "print(f'  Test Accuracy: {acc:.4f}')\n",
    "print(f'  Sampling frequency: {int(sampling_freq) if not math.isnan(sampling_freq) else \"N/A\"} Hz')\n",
    "print(f'  Device used: {DEVICE}\\n')\n",
    "print('Label mapping:')\n",
    "print('0: Normal Sinus Rhythm (NORM)')\n",
    "print('1: Atrial Fibrillation (AFIB)\\n')\n",
    "print('Classification Report:')\n",
    "print(clf_report)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# human-readable confusion breakdown\n",
    "if cm.size == 4:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total_norm = tn + fp\n",
    "    total_af = fn + tp\n",
    "    print(f\"{tn} Normal ECGs correctly classified ({(tn/total_norm*100) if total_norm>0 else 0:.1f}%)\")\n",
    "    print(f\"{fp} Normal ECGs wrongly predicted as AFIB ({(fp/total_norm*100) if total_norm>0 else 0:.1f}%) [False Positives]\")\n",
    "    print(f\"{tp} AFIB ECGs correctly classified ({(tp/total_af*100) if total_af>0 else 0:.1f}%)\")\n",
    "    print(f\"{fn} AFIB ECGs wrongly predicted as non-AFIB ({(fn/total_af*100) if total_af>0 else 0:.1f}%) [False Negatives]\\n\")\n",
    "\n",
    "print('Dataset Sizes:')\n",
    "print(f'  Training records:   {train_n}')\n",
    "print(f'  Validation records: {val_n}')\n",
    "print(f'  Test records:       {test_n}\\n')\n",
    "print('Runtime:', runtime_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Test Metrics Summary to a timestamped file in `results/`\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('results')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = f\"run_{now.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "out_path = out_dir / filename\n",
    "\n",
    "# Build the summary text (reuse variables computed in the evaluation cell)\n",
    "lines = []\n",
    "lines.append('=== Test Metrics Summary ===')\n",
    "try:\n",
    "    lines.append(f'  Test Loss:     {test_loss:.4f}')\n",
    "except Exception:\n",
    "    lines.append('  Test Loss:     N/A')\n",
    "try:\n",
    "    lines.append(f'  Test F1-score: {f1:.4f}')\n",
    "except Exception:\n",
    "    lines.append('  Test F1-score: N/A')\n",
    "try:\n",
    "    lines.append(f'  Test Accuracy: {acc:.4f}')\n",
    "except Exception:\n",
    "    lines.append('  Test Accuracy: N/A')\n",
    "\n",
    "# sampling frequency\n",
    "try:\n",
    "    sf_text = int(sampling_freq) if not math.isnan(sampling_freq) else 'N/A'\n",
    "except Exception:\n",
    "    sf_text = 'N/A'\n",
    "lines.append(f'  Sampling frequency: {sf_text} Hz')\n",
    "lines.append(f'  Device used: {DEVICE}\\n')\n",
    "\n",
    "lines.append('Label mapping:')\n",
    "lines.append('0: Normal Sinus Rhythm (NORM)')\n",
    "lines.append('1: Atrial Fibrillation (AFIB)\\n')\n",
    "\n",
    "lines.append('Classification Report:')\n",
    "try:\n",
    "    lines.append(clf_report)\n",
    "except Exception:\n",
    "    lines.append('N/A')\n",
    "\n",
    "lines.append('\\nConfusion Matrix:')\n",
    "try:\n",
    "    lines.append(str(cm))\n",
    "except Exception:\n",
    "    lines.append('N/A')\n",
    "\n",
    "# human-readable confusion breakdown\n",
    "try:\n",
    "    if cm.size == 4:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total_norm = tn + fp\n",
    "        total_af = fn + tp\n",
    "        lines.append(f\"{tn} Normal ECGs correctly classified ({(tn/total_norm*100) if total_norm>0 else 0:.1f}%)\")\n",
    "        lines.append(f\"{fp} Normal ECGs wrongly predicted as AFIB ({(fp/total_norm*100) if total_norm>0 else 0:.1f}%) [False Positives]\")\n",
    "        lines.append(f\"{tp} AFIB ECGs correctly classified ({(tp/total_af*100) if total_af>0 else 0:.1f}%)\")\n",
    "        lines.append(f\"{fn} AFIB ECGs wrongly predicted as non-AFIB ({(fn/total_af*100) if total_af>0 else 0:.1f}%) [False Negatives]\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "lines.append('Dataset Sizes:')\n",
    "try:\n",
    "    lines.append(f'  Training records:   {train_n}')\n",
    "    lines.append(f'  Validation records: {val_n}')\n",
    "    lines.append(f'  Test records:       {test_n}\\n')\n",
    "except Exception:\n",
    "    lines.append('  Training records:   N/A')\n",
    "    lines.append('  Validation records: N/A')\n",
    "    lines.append('  Test records:       N/A\\n')\n",
    "\n",
    "# runtime\n",
    "try:\n",
    "    lines.append(f'Runtime: {runtime_str}')\n",
    "except Exception:\n",
    "    lines.append('Runtime: N/A')\n",
    "\n",
    "# Write to file\n",
    "with open(out_path, 'w', encoding='utf-8') as fh:\n",
    "    fh.write('\\n'.join(lines))\n",
    "\n",
    "print(f'Saved Test Metrics Summary to: {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
