{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-AFIB', 'AFIB'],\n",
    "            yticklabels=['Non-AFIB', 'AFIB'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Test Set)\\nAUC: {test_auc:.4f}, Acc: {test_acc:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ AFIB detection pipeline complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a47df",
   "metadata": {},
   "source": [
    "## 14. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from {MODEL_PATH}\")\n",
    "print(f\"\\nEvaluating on test set...\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = 0.0\n",
    "all_test_preds = []\n",
    "all_test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions\n",
    "        probs = torch.sigmoid(output).cpu().numpy()\n",
    "        all_test_preds.extend(probs)\n",
    "        all_test_targets.extend(target.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Convert to arrays\n",
    "all_test_preds = np.array(all_test_preds)\n",
    "all_test_targets = np.array(all_test_targets)\n",
    "\n",
    "# Compute metrics\n",
    "test_auc = roc_auc_score(all_test_targets, all_test_preds)\n",
    "test_pred_labels = (all_test_preds > 0.5).astype(int)\n",
    "test_acc = accuracy_score(all_test_targets, test_pred_labels)\n",
    "conf_matrix = confusion_matrix(all_test_targets, test_pred_labels)\n",
    "\n",
    "print(f\"--- Test Set Results ---\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_targets, test_pred_labels, target_names=['Non-AFIB', 'AFIB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afbecf",
   "metadata": {},
   "source": [
    "## 13. Load Best Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cfec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: AUC\n",
    "axes[1].plot(history['val_auc'], label='Val AUC', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUC')\n",
    "axes[1].set_title('Validation AUC')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot 3: Accuracy\n",
    "axes[2].plot(history['val_acc'], label='Val Accuracy', color='orange')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Validation Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17343b7",
   "metadata": {},
   "source": [
    "## 12. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa91d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_auc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_auc = 0.0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========== Training phase ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()  # Shape: (batch,)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # ========== Validation phase ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions (apply sigmoid to get probabilities)\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Compute metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    val_auc = roc_auc_score(all_targets, all_preds)\n",
    "    val_acc = accuracy_score(all_targets, (all_preds > 0.5).astype(int))\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val AUC: {val_auc:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  → Best model saved (AUC: {val_auc:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb29b7",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf90be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"\\n✓ Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d8faf",
   "metadata": {},
   "source": [
    "## 10. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG1DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 1D CNN for AFIB detection from 12-lead ECG.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=12, num_classes=1):\n",
    "        super(ECG1DCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 12, T)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Global pooling: (batch, 512, T') -> (batch, 512, 1)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)  # (batch, 512)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)  # (batch, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = ECG1DCNN(in_channels=12, num_classes=1)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"\\n✓ Model initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a4d9b",
   "metadata": {},
   "source": [
    "## 9. Define 1D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds = ECGDataset(\n",
    "    cleaned_mapping, \n",
    "    idx_train, \n",
    "    downsample=DOWNSAMPLE, \n",
    "    target_col=TARGET_COL,\n",
    "    channel_mean=global_mean, \n",
    "    channel_std=global_std\n",
    ")\n",
    "\n",
    "val_ds = ECGDataset(\n",
    "    cleaned_mapping, \n",
    "    idx_val, \n",
    "    downsample=DOWNSAMPLE, \n",
    "    target_col=TARGET_COL,\n",
    "    channel_mean=global_mean, \n",
    "    channel_std=global_std\n",
    ")\n",
    "\n",
    "test_ds = ECGDataset(\n",
    "    cleaned_mapping, \n",
    "    idx_test, \n",
    "    downsample=DOWNSAMPLE, \n",
    "    target_col=TARGET_COL,\n",
    "    channel_mean=global_mean, \n",
    "    channel_std=global_std\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader:   {len(val_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_loader)} batches\")\n",
    "print(f\"\\n✓ DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7226dc5",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ca6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for ECG data that loads from .npy files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mapping, indices, downsample=None, target_col=\"_AFIB\", \n",
    "                 channel_mean=None, channel_std=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mapping: DataFrame with columns ['record_id', 'ecg_path', target_col]\n",
    "            indices: Array of indices to include in this dataset\n",
    "            downsample: Downsample factor (None or int > 1)\n",
    "            target_col: Name of the target column\n",
    "            channel_mean: Per-channel mean for normalization (shape: 12,)\n",
    "            channel_std: Per-channel std for normalization (shape: 12,)\n",
    "        \"\"\"\n",
    "        self.mapping = mapping.iloc[indices].reset_index(drop=True)\n",
    "        self.downsample = downsample\n",
    "        self.target_col = target_col\n",
    "        self.channel_mean = channel_mean\n",
    "        self.channel_std = channel_std\n",
    "        \n",
    "        # Ensure mean/std are available\n",
    "        if self.channel_mean is None:\n",
    "            self.channel_mean = np.zeros(12, dtype=np.float32)\n",
    "        if self.channel_std is None:\n",
    "            self.channel_std = np.ones(12, dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.mapping.iloc[idx]\n",
    "        record_id = str(row[\"record_id\"])\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        \n",
    "        # Load ECG data\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            \n",
    "            # Ensure 2D array\n",
    "            if data.ndim != 2:\n",
    "                raise ValueError(f\"Expected 2D array, got shape {data.shape}\")\n",
    "            \n",
    "            # If shape is (L, 12), transpose to (12, L)\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            \n",
    "            # Should now be (12, L)\n",
    "            if data.shape[0] != 12:\n",
    "                raise ValueError(f\"Expected 12 channels, got {data.shape[0]}\")\n",
    "            \n",
    "            # Apply downsampling if specified\n",
    "            if self.downsample is not None and self.downsample > 1:\n",
    "                data = data[:, ::self.downsample]\n",
    "            \n",
    "            # Normalize: (data - mean) / std\n",
    "            # mean and std have shape (12,), data has shape (12, T)\n",
    "            data = (data - self.channel_mean[:, None]) / self.channel_std[:, None]\n",
    "            \n",
    "            # Convert to float32\n",
    "            data = data.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {record_id}: {e}\")\n",
    "            # Return zeros as fallback\n",
    "            data = np.zeros((12, 2500), dtype=np.float32)\n",
    "        \n",
    "        # Build label\n",
    "        try:\n",
    "            tval = float(row[self.target_col])\n",
    "            label = 1.0 if tval > 0 else 0.0\n",
    "        except:\n",
    "            label = 0.0\n",
    "        \n",
    "        # Convert to tensors\n",
    "        data_tensor = torch.from_numpy(data).float()\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "print(\"✓ ECGDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac61ee6",
   "metadata": {},
   "source": [
    "## 7. Define ECG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global statistics from training set\n",
    "print(\"Computing global channel statistics from training data...\")\n",
    "global_mean, global_std = compute_channel_stats(\n",
    "    cleaned_mapping, \n",
    "    idx_train, \n",
    "    downsample=DOWNSAMPLE,\n",
    "    max_samples=512\n",
    ")\n",
    "\n",
    "print(f\"\\nGlobal mean (per channel): {global_mean}\")\n",
    "print(f\"Global std (per channel):  {global_std}\")\n",
    "print(f\"\\n✓ Channel statistics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(mapping, indices, downsample=None, max_samples=512):\n",
    "    \"\"\"\n",
    "    Compute global per-channel mean and std from a subset of training data.\n",
    "    \n",
    "    Args:\n",
    "        mapping: DataFrame with 'ecg_path' column\n",
    "        indices: Array of indices to use\n",
    "        downsample: Downsample factor (None or int > 1)\n",
    "        max_samples: Maximum number of samples to load for statistics\n",
    "    \n",
    "    Returns:\n",
    "        mean, std: Arrays of shape (12,) with per-channel statistics\n",
    "    \"\"\"\n",
    "    # Randomly sample up to max_samples indices\n",
    "    if len(indices) > max_samples:\n",
    "        sample_indices = np.random.choice(indices, size=max_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = indices\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        row = mapping.iloc[int(idx)]\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        \n",
    "        if not npy_file.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load ECG data\n",
    "            data = np.load(npy_file)\n",
    "            \n",
    "            # Ensure 2D array\n",
    "            if data.ndim != 2:\n",
    "                continue\n",
    "            \n",
    "            # If shape is (L, 12), transpose to (12, L)\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            \n",
    "            # Should now be (12, L)\n",
    "            if data.shape[0] != 12:\n",
    "                continue\n",
    "            \n",
    "            # Apply downsampling if specified\n",
    "            if downsample is not None and downsample > 1:\n",
    "                data = data[:, ::downsample]\n",
    "            \n",
    "            data_list.append(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to load {npy_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not data_list:\n",
    "        raise ValueError(\"No valid data loaded for computing statistics\")\n",
    "    \n",
    "    # Stack into (N, 12, T)\n",
    "    # Note: T may vary, so we'll compute stats per sample then average\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    \n",
    "    for data in data_list:\n",
    "        # data shape: (12, T)\n",
    "        all_means.append(data.mean(axis=1))  # Shape: (12,)\n",
    "        all_stds.append(data.std(axis=1))     # Shape: (12,)\n",
    "    \n",
    "    # Average across samples\n",
    "    mean = np.mean(all_means, axis=0).astype(np.float32)\n",
    "    std = np.mean(all_stds, axis=0).astype(np.float32) + 1e-6\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "print(\"✓ Function defined: compute_channel_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f011e",
   "metadata": {},
   "source": [
    "## 6. Compute Global Channel Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44736d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices array\n",
    "indices = np.arange(len(cleaned_mapping))\n",
    "labels = cleaned_mapping[TARGET_COL].values\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "idx_train, idx_temp, y_train, y_temp = train_test_split(\n",
    "    indices, labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: temp into 50% val, 50% test\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"--- Dataset Splits ---\")\n",
    "print(f\"Train: {len(idx_train):,} samples (AFIB: {y_train.sum():,}, Non-AFIB: {(y_train == 0).sum():,})\")\n",
    "print(f\"Val:   {len(idx_val):,} samples (AFIB: {y_val.sum():,}, Non-AFIB: {(y_val == 0).sum():,})\")\n",
    "print(f\"Test:  {len(idx_test):,} samples (AFIB: {y_test.sum():,}, Non-AFIB: {(y_test == 0).sum():,})\")\n",
    "print(f\"\\n✓ Dataset split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce1025",
   "metadata": {},
   "source": [
    "## 5. Split Dataset (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ddae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use _AFIB as the primary target column\n",
    "TARGET_COL = \"_AFIB\"\n",
    "\n",
    "# Ensure target is strictly 0/1\n",
    "cleaned_mapping[TARGET_COL] = pd.to_numeric(cleaned_mapping[TARGET_COL], errors='coerce').fillna(0)\n",
    "cleaned_mapping[TARGET_COL] = (cleaned_mapping[TARGET_COL] > 0).astype(int)\n",
    "\n",
    "print(f\"Target column: {TARGET_COL}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(cleaned_mapping[TARGET_COL].value_counts().sort_index())\n",
    "print(f\"\\n✓ Labels prepared for binary AFIB detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24226e4f",
   "metadata": {},
   "source": [
    "## 4. Prepare Labels for Binary AFIB Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f699eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned mapping\n",
    "cleaned_mapping = pd.read_csv(MAPPING_CSV)\n",
    "\n",
    "print(f\"Loaded {len(cleaned_mapping):,} records from {MAPPING_CSV.name}\")\n",
    "print(f\"\\nColumns: {list(cleaned_mapping.columns)}\")\n",
    "\n",
    "# Verify required columns exist\n",
    "required_cols = ['record_id', 'ecg_path', '_AFIB', '_SR']\n",
    "missing_cols = [col for col in required_cols if col not in cleaned_mapping.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"AFIB records (_AFIB=1): {cleaned_mapping['_AFIB'].sum():,}\")\n",
    "print(f\"Non-AFIB records (_AFIB=0): {(cleaned_mapping['_AFIB'] == 0).sum():,}\")\n",
    "print(f\"SR records (_SR=1): {cleaned_mapping['_SR'].sum():,}\")\n",
    "print(f\"Non-SR records (_SR=0): {(cleaned_mapping['_SR'] == 0).sum():,}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(f\"\\nSample rows:\")\n",
    "cleaned_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bce11c",
   "metadata": {},
   "source": [
    "## 3. Load Cleaned Mapping & Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562288ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DOWNSAMPLE = 2  # Downsample factor (e.g., 2 = keep every 2nd sample)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths (all relative to project root)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CLEANED_ROOT = DATA_DIR / \"cleaned_balanced_AFIB_SR\"\n",
    "CLEANED_WFDB_DIR = CLEANED_ROOT / \"WFDBRecords\"\n",
    "MAPPING_CSV = CLEANED_ROOT / \"file_mapping_cleaned.csv\"\n",
    "\n",
    "# Model save path\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"ecg_1dcnn_afib_balanced.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"MAPPING_CSV: {MAPPING_CSV}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(f\"\\n✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc1429",
   "metadata": {},
   "source": [
    "## 2. Configuration & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd7335",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03ef0e",
   "metadata": {},
   "source": [
    "# AFIB Detection Using 1D CNN on Cleaned ECG Dataset\n",
    "\n",
    "This notebook trains a binary AFIB detector using the cleaned and balanced dataset generated by data_handling.ipynb.\n",
    "\n",
    "**Workflow:**\n",
    "1. Setup paths and load cleaned mapping\n",
    "2. Verify data and prepare labels\n",
    "3. Split dataset (train/val/test)\n",
    "4. Compute global channel statistics\n",
    "5. Define ECG dataset class\n",
    "6. Create data loaders\n",
    "7. Define and train 1D CNN model\n",
    "8. Evaluate on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
