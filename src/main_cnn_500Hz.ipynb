{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f698a841",
   "metadata": {},
   "source": [
    "# AFIB Detection Using 1D CNN on Cleaned ECG Dataset\n",
    "\n",
    "This notebook trains a binary AFIB detector using the cleaned and balanced dataset generated by data_handling.ipynb.\n",
    "\n",
    "Workflow:\n",
    "1. Setup paths and load cleaned mapping\n",
    "2. Verify data and prepare labels\n",
    "3. Split dataset (train/val/test)\n",
    "4. Compute global channel statistics\n",
    "5. Define ECG dataset class\n",
    "6. Create data loaders\n",
    "7. Define and train 1D CNN model\n",
    "8. Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f00e5f",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✓ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118822cc",
   "metadata": {},
   "source": [
    "## 2. Configuration & Path Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DOWNSAMPLE = 1  # Downsample factor (e.g., 2 = keep every 2nd sample)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths (all relative to project root)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CLEANED_ROOT = DATA_DIR / \"cleaned_balanced_AFIB_SR\"\n",
    "CLEANED_WFDB_DIR = CLEANED_ROOT / \"WFDBRecords\"\n",
    "MAPPING_CSV = CLEANED_ROOT / \"file_mapping_cleaned_with_folds.csv\"\n",
    "\n",
    "# Model save path (main_2)\n",
    "MODEL_DIR = PROJECT_ROOT / \"cnn_models\"/\"500Hz\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"ecg_1dcnn_afib_balanced_500Hz.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"MAPPING_CSV: {MAPPING_CSV}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(\"\\n✓ Configuration complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feced3c",
   "metadata": {},
   "source": [
    "## 3. Load Cleaned Mapping & Verify Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed79a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned mapping\n",
    "cleaned_mapping = pd.read_csv(MAPPING_CSV)\n",
    "\n",
    "print(f\"Loaded {len(cleaned_mapping):,} records from {MAPPING_CSV.name}\")\n",
    "print(f\"\\nColumns: {list(cleaned_mapping.columns)}\")\n",
    "\n",
    "# Verify required columns exist\n",
    "required_cols = ['record_id', 'ecg_path', '_AFIB', '_SR', 'fold']\n",
    "missing_cols = [col for col in required_cols if col not in cleaned_mapping.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"AFIB records (_AFIB=1): {cleaned_mapping['_AFIB'].sum():,}\")\n",
    "print(f\"Non-AFIB records (_AFIB=0): {(cleaned_mapping['_AFIB'] == 0).sum():,}\")\n",
    "print(f\"SR records (_SR=1): {cleaned_mapping['_SR'].sum():,}\")\n",
    "print(f\"Non-SR records (_SR=0): {(cleaned_mapping['_SR'] == 0).sum():,}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample rows:\")\n",
    "cleaned_mapping.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f7a44",
   "metadata": {},
   "source": [
    "## 4. Prepare Labels for Binary AFIB Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddadc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use _AFIB as the primary target column\n",
    "target_col = \"_AFIB\"\n",
    "\n",
    "# Ensure target is strictly 0/1\n",
    "cleaned_mapping[target_col] = pd.to_numeric(cleaned_mapping[target_col], errors='coerce').fillna(0)\n",
    "cleaned_mapping[target_col] = (cleaned_mapping[target_col] > 0).astype(int)\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(\"Target distribution:\")\n",
    "print(cleaned_mapping[target_col].value_counts().sort_index())\n",
    "print(\"\\n✓ Labels prepared for binary AFIB detection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02ea56",
   "metadata": {},
   "source": [
    "## 5. Split Dataset (Train/Val/Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset using precomputed patient-level folds (no leakage)\n",
    "all_folds = sorted(cleaned_mapping['fold'].unique())\n",
    "test_folds = [0, 1]  # outer test folds (~40% here because 2/5); adjust as needed\n",
    "train_folds = [f for f in all_folds if f not in test_folds]\n",
    "val_fold = train_folds[-1]  # use the last training fold for validation\n",
    "train_only_folds = [f for f in train_folds if f != val_fold]\n",
    "\n",
    "train_mask = cleaned_mapping['fold'].isin(train_only_folds)\n",
    "val_mask = cleaned_mapping['fold'] == val_fold\n",
    "test_mask = cleaned_mapping['fold'].isin(test_folds)\n",
    "\n",
    "idx_train = cleaned_mapping.index[train_mask].to_numpy()\n",
    "idx_val = cleaned_mapping.index[val_mask].to_numpy()\n",
    "idx_test = cleaned_mapping.index[test_mask].to_numpy()\n",
    "\n",
    "y_train = cleaned_mapping.loc[idx_train, target_col].to_numpy()\n",
    "y_val = cleaned_mapping.loc[idx_val, target_col].to_numpy()\n",
    "y_test = cleaned_mapping.loc[idx_test, target_col].to_numpy()\n",
    "\n",
    "print(\"--- Dataset Splits (by fold) ---\")\n",
    "print(f\"All folds: {all_folds}\")\n",
    "print(f\"Train folds: {train_only_folds}\")\n",
    "print(f\"Val fold: {val_fold}\")\n",
    "print(f\"Test folds: {test_folds}\")\n",
    "print(f\"Train: {len(idx_train):,} samples (AFIB: {y_train.sum():,}, Non-AFIB: {(y_train == 0).sum():,})\")\n",
    "print(f\"Val:   {len(idx_val):,} samples (AFIB: {y_val.sum():,}, Non-AFIB: {(y_val == 0).sum():,})\")\n",
    "print(f\"Test:  {len(idx_test):,} samples (AFIB: {y_test.sum():,}, Non-AFIB: {(y_test == 0).sum():,})\")\n",
    "print(\"\\n✓ Dataset split complete (fold-based)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a78d8",
   "metadata": {},
   "source": [
    "## 6. Compute Global Channel Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(mapping, indices, downsample=None):\n",
    "    \"\"\"\n",
    "    Compute global per-channel mean and std from a subset of training data.\n",
    "    Stacks up to 512 random samples to estimate statistics.\n",
    "    \"\"\"\n",
    "    if len(indices) > 512:\n",
    "        sample_indices = np.random.choice(indices, size=512, replace=False)\n",
    "    else:\n",
    "        sample_indices = indices\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = mapping.iloc[int(idx)]\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        if not npy_file.exists():\n",
    "            continue\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Ensure 2D array and shape (12, L)\n",
    "            if data.ndim != 2:\n",
    "                continue\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            if data.shape[0] != 12:\n",
    "                continue\n",
    "            if downsample is not None and downsample > 1:\n",
    "                data = data[:, ::downsample]\n",
    "            data_list.append(data)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not data_list:\n",
    "        raise ValueError(\"No valid data loaded for computing statistics\")\n",
    "\n",
    "    # Align lengths to stack\n",
    "    min_len = min(arr.shape[1] for arr in data_list)\n",
    "    data_list = [arr[:, :min_len] for arr in data_list]\n",
    "    stack = np.stack(data_list, axis=0)  # (N, 12, T)\n",
    "\n",
    "    mean = stack.mean(axis=(0, 2)).astype(np.float32)\n",
    "    std = (stack.std(axis=(0, 2)) + 1e-6).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "print(\"✓ Function defined: compute_channel_stats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global statistics from training set\n",
    "print(\"Computing global channel statistics from training data...\")\n",
    "global_mean, global_std = compute_channel_stats(\n",
    "    cleaned_mapping,\n",
    "    idx_train,\n",
    "    downsample=DOWNSAMPLE,\n",
    ")\n",
    "\n",
    "print(f\"\\nGlobal mean (per channel): {global_mean}\")\n",
    "print(f\"Global std (per channel):  {global_std}\")\n",
    "print(\"\\n✓ Channel statistics computed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bccc68",
   "metadata": {},
   "source": [
    "## 7. Define ECG Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60db36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ECG data that loads from .npy files.\"\"\"\n",
    "    def __init__(self, mapping, indices, downsample=None, target_col=\"_AFIB\",\n",
    "                 channel_mean=None, channel_std=None):\n",
    "        self.mapping = mapping.iloc[indices].reset_index(drop=True)\n",
    "        self.downsample = downsample\n",
    "        self.target_col = target_col\n",
    "        self.channel_mean = channel_mean if channel_mean is not None else np.zeros(12, dtype=np.float32)\n",
    "        self.channel_std = channel_std if channel_std is not None else np.ones(12, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.mapping.iloc[idx]\n",
    "        record_id = str(row[\"record_id\"])\n",
    "        # Resolve path robustly: support relative paths stored in CSV\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        if not npy_file.is_absolute():\n",
    "            npy_file = (PROJECT_ROOT / npy_file).resolve()\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Ensure 2D array and shape (12, L)\n",
    "            if data.ndim != 2:\n",
    "                raise ValueError(f\"Expected 2D array, got shape {data.shape}\")\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            if data.shape[0] != 12:\n",
    "                raise ValueError(f\"Expected 12 channels, got {data.shape[0]}\")\n",
    "            if self.downsample is not None and self.downsample > 1:\n",
    "                data = data[:, ::self.downsample]\n",
    "            # Normalize\n",
    "            data = (data - self.channel_mean[:, None]) / self.channel_std[:, None]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {record_id}: {e}\")\n",
    "            # Fallback dummy tensor (kept simple)\n",
    "            data = np.zeros((12, 2500), dtype=np.float32)\n",
    "        try:\n",
    "            tval = float(row[self.target_col])\n",
    "            label = 1.0 if tval > 0 else 0.0\n",
    "        except Exception:\n",
    "            label = 0.0\n",
    "        data_tensor = torch.from_numpy(data.astype(np.float32))\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "print(\"✓ ECGDataset class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf830",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09968898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds = ECGDataset(cleaned_mapping, idx_train, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                       channel_mean=global_mean, channel_std=global_std)\n",
    "val_ds = ECGDataset(cleaned_mapping, idx_val, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                     channel_mean=global_mean, channel_std=global_std)\n",
    "test_ds = ECGDataset(cleaned_mapping, idx_test, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                      channel_mean=global_mean, channel_std=global_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader:   {len(val_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_loader)} batches\")\n",
    "print(\"\\n✓ DataLoaders created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f259028",
   "metadata": {},
   "source": [
    "## 9. Define 1D CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG1DCNN(nn.Module):\n",
    "    def __init__(self, in_channels=12, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ECG1DCNN(in_channels=12, num_classes=1).to(DEVICE)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"\\n✓ Model initialized on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc50dd",
   "metadata": {},
   "source": [
    "## 10. Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(\"\\n✓ Training setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bd4c0",
   "metadata": {},
   "source": [
    "## 11. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d90012",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_acc': []}\n",
    "best_val_auc = 0.0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\\n\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / max(1, len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    avg_val_loss = val_loss / max(1, len(val_loader))\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    val_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.5\n",
    "    val_acc = accuracy_score(all_targets, (all_preds > 0.5).astype(int))\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  → Best model saved (AUC: {val_auc:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560cf22",
   "metadata": {},
   "source": [
    "## 12. Plot Training History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].plot(history['val_auc'], label='Val AUC', color='green')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('AUC'); axes[1].set_title('Validation AUC')\n",
    "axes[1].legend(); axes[1].grid(True)\n",
    "axes[2].plot(history['val_acc'], label='Val Accuracy', color='orange')\n",
    "axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('Accuracy'); axes[2].set_title('Validation Accuracy')\n",
    "axes[2].legend(); axes[2].grid(True)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96861a5a",
   "metadata": {},
   "source": [
    "## 13. Load Best Model & Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from {MODEL_PATH}\")\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "\n",
    "test_loss = 0.0\n",
    "all_test_preds, all_test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        probs = torch.sigmoid(output).cpu().numpy()\n",
    "        all_test_preds.extend(probs)\n",
    "        all_test_targets.extend(target.cpu().numpy())\n",
    "avg_test_loss = test_loss / max(1, len(test_loader))\n",
    "all_test_preds = np.array(all_test_preds)\n",
    "all_test_targets = np.array(all_test_targets)\n",
    "\n",
    "test_auc = roc_auc_score(all_test_targets, all_test_preds) if len(np.unique(all_test_targets)) > 1 else 0.5\n",
    "test_pred_labels = (all_test_preds > 0.5).astype(int)\n",
    "test_acc = accuracy_score(all_test_targets, test_pred_labels)\n",
    "conf_matrix = confusion_matrix(all_test_targets, test_pred_labels)\n",
    "\n",
    "print(\"--- Test Set Results ---\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_targets, test_pred_labels, target_names=['Non-AFIB', 'AFIB']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2eebf",
   "metadata": {},
   "source": [
    "## 14. Visualize Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c54510",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-AFIB', 'AFIB'],\n",
    "            yticklabels=['Non-AFIB', 'AFIB'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Test Set)\\nAUC: {test_auc:.4f}, Acc: {test_acc:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"✓ AFIB detection pipeline complete!\")\n",
    "print(\"============================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea15b8",
   "metadata": {},
   "source": [
    "## 15. ROC Curve (Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_test_targets, all_test_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a626b",
   "metadata": {},
   "source": [
    "## 16. DET Curve (Detection Error Tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd652b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import DetCurveDisplay\n",
    "\n",
    "# Create and plot DET curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "DetCurveDisplay.from_predictions(all_test_targets, all_test_preds, ax=ax, name='AFIB Detection')\n",
    "\n",
    "ax.set_title('Detection Error Tradeoff (DET) Curve', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('False Negative Rate', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DET Curve: Lower values indicate better performance\")\n",
    "print(f\"The curve shows the tradeoff between false positives and false negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f60d0",
   "metadata": {},
   "source": [
    "# 17. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# Choose source: prefer test-set outputs, fallback to validation\n",
    "if 'all_test_targets' in globals() and 'all_test_preds' in globals():\n",
    "    y_true = np.array(all_test_targets)\n",
    "    y_prob = np.array(all_test_preds)\n",
    "elif 'all_targets' in globals() and 'all_preds' in globals():\n",
    "    y_true = np.array(all_targets)\n",
    "    y_prob = np.array(all_preds)\n",
    "else:\n",
    "    print(\"No predictions found. Run the evaluation cell (Section 13) or the training loop first.\")\n",
    "    y_true = np.array([])\n",
    "    y_prob = np.array([])\n",
    "\n",
    "if y_true.size > 0 and y_prob.size > 0:\n",
    "    # Binary predictions at 0.5 threshold\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    # Core metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pre = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Specificity (True Negative Rate) with stable 2x2 matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "\n",
    "    # ROC-AUC using probabilities\n",
    "    roc_auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.5\n",
    "\n",
    "    print(f\"Acc:  {acc:.3f}\")\n",
    "    print(f\"F1:   {f1:.3f}\")\n",
    "    print(f\"Pre:  {pre:.3f}\")\n",
    "    print(f\"Rec:  {rec:.3f}\")\n",
    "    print(f\"Spec: {spec:.3f}\")\n",
    "    print(f\"MCC:  {mcc:.3f}\")\n",
    "    print(f\"AUC:  {roc_auc:.3f}\")\n",
    "else:\n",
    "    print(\"Metrics not computed because prediction arrays are empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results to results folder\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Calculate sample rate (original 500 Hz downsampled by factor DOWNSAMPLE)\n",
    "original_sample_rate = 500  # Hz\n",
    "sample_rate = original_sample_rate / DOWNSAMPLE  # Hz\n",
    "\n",
    "# Generate results file\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_name = \"1D-CNN\"\n",
    "results_filename = RESULTS_DIR / f\"results_{model_name}_{timestamp}.txt\"\n",
    "\n",
    "# Format results text\n",
    "results_text = f\"\"\"\n",
    "================================================================================\n",
    "AFIB Detection Model Results\n",
    "================================================================================\n",
    "Model Name:        {model_name}\n",
    "Sample Rate:       {sample_rate:.1f} Hz (Original: {original_sample_rate} Hz, Downsample: {DOWNSAMPLE}x)\n",
    "Batch Size:        {BATCH_SIZE}\n",
    "Learning Rate:     {LEARNING_RATE}\n",
    "Num Epochs:        {NUM_EPOCHS}\n",
    "Device:            {DEVICE}\n",
    "Timestamp:         {timestamp}\n",
    "\n",
    "================================================================================\n",
    "Test Set Metrics\n",
    "================================================================================\n",
    "Accuracy (Acc):    {acc:.4f}\n",
    "F1-Score (F1):     {f1:.4f}\n",
    "Precision (Pre):   {pre:.4f}\n",
    "Recall (Rec):      {rec:.4f}\n",
    "Specificity (Spec):{spec:.4f}\n",
    "Matthews Corr Coef:{mcc:.4f}\n",
    "ROC-AUC (AUC):     {roc_auc:.4f}\n",
    "\n",
    "Test Loss:         {avg_test_loss:.4f}\n",
    "\n",
    "================================================================================\n",
    "Confusion Matrix\n",
    "================================================================================\n",
    "                  Predicted\n",
    "               Non-AFIB  AFIB\n",
    "Actual Non-AFIB    {TN:>4d}    {FP:>4d}\n",
    "       AFIB        {FN:>4d}    {TP:>4d}\n",
    "\n",
    "TP (True Positive):   {TP}\n",
    "TN (True Negative):   {TN}\n",
    "FP (False Positive):  {FP}\n",
    "FN (False Negative):  {FN}\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open(results_filename, 'w') as f:\n",
    "    f.write(results_text)\n",
    "\n",
    "print(f\"✓ Results saved to: {results_filename}\")\n",
    "print(results_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a379da",
   "metadata": {},
   "source": [
    "## 18. Per-Fold Performance Analysis\n",
    "\n",
    "Evaluate model performance on each individual fold to ensure no patient leakage and assess generalization across different patient groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no patient leakage between folds\n",
    "print(\"=== Verifying Patient-Level Fold Assignment ===\\n\")\n",
    "\n",
    "patient_fold_counts = cleaned_mapping.groupby('patient_id')['fold'].nunique()\n",
    "patients_in_multiple_folds = patient_fold_counts[patient_fold_counts > 1]\n",
    "\n",
    "print(f\"Total unique patients: {cleaned_mapping['patient_id'].nunique():,}\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"Patients appearing in multiple folds: {len(patients_in_multiple_folds)}\")\n",
    "\n",
    "if len(patients_in_multiple_folds) > 0:\n",
    "    print(\"\\n⚠️ WARNING: Patient leakage detected!\")\n",
    "    print(\"\\nPatients in multiple folds:\")\n",
    "    for patient, count in patients_in_multiple_folds.items():\n",
    "        folds = cleaned_mapping[cleaned_mapping['patient_id'] == patient]['fold'].unique()\n",
    "        print(f\"  {patient}: appears in {count} folds -> {sorted(folds)}\")\n",
    "else:\n",
    "    print(\"\\n✅ NO PATIENT LEAKAGE DETECTED\")\n",
    "    print(\"   Each patient appears in exactly one fold - proper stratification confirmed!\")\n",
    "\n",
    "# Distribution across folds\n",
    "print(\"\\n=== Fold Distribution Summary ===\")\n",
    "for fold in sorted(cleaned_mapping['fold'].unique()):\n",
    "    fold_data = cleaned_mapping[cleaned_mapping['fold'] == fold]\n",
    "    n_records = len(fold_data)\n",
    "    n_patients = fold_data['patient_id'].nunique()\n",
    "    n_afib = fold_data['_AFIB'].sum()\n",
    "    n_sr = (fold_data['_AFIB'] == 0).sum()\n",
    "    print(f\"Fold {fold}: {n_records:4d} records ({n_patients:4d} patients) | AFIB: {n_afib:3d} ({n_afib/n_records*100:.1f}%) | SR: {n_sr:3d} ({n_sr/n_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on each fold separately\n",
    "print(\"\\n=== Per-Fold Performance Evaluation ===\\n\")\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold in sorted(cleaned_mapping['fold'].unique()):\n",
    "    # Get indices for this fold\n",
    "    fold_mask = cleaned_mapping['fold'] == fold\n",
    "    idx_fold = cleaned_mapping.index[fold_mask].to_numpy()\n",
    "    \n",
    "    # Create dataset and loader for this fold\n",
    "    fold_ds = ECGDataset(cleaned_mapping, idx_fold, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                         channel_mean=global_mean, channel_std=global_std)\n",
    "    fold_loader = DataLoader(fold_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Evaluate on this fold\n",
    "    model.eval()\n",
    "    fold_preds, fold_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in fold_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data).squeeze()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            fold_preds.extend(probs)\n",
    "            fold_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    fold_preds = np.array(fold_preds)\n",
    "    fold_targets = np.array(fold_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_auc = roc_auc_score(fold_targets, fold_preds) if len(np.unique(fold_targets)) > 1 else 0.5\n",
    "    fold_pred_labels = (fold_preds > 0.5).astype(int)\n",
    "    fold_acc = accuracy_score(fold_targets, fold_pred_labels)\n",
    "    fold_cm = confusion_matrix(fold_targets, fold_pred_labels)\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'n_samples': len(fold_targets),\n",
    "        'auc': fold_auc,\n",
    "        'accuracy': fold_acc,\n",
    "        'confusion_matrix': fold_cm\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"  Samples: {len(fold_targets):4d}\")\n",
    "    print(f\"  AUC:     {fold_auc:.4f}\")\n",
    "    print(f\"  Accuracy: {fold_acc:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\")\n",
    "    print(f\"    TN={fold_cm[0,0]:3d}  FP={fold_cm[0,1]:3d}\")\n",
    "    print(f\"    FN={fold_cm[1,0]:3d}  TP={fold_cm[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    if fold_cm[1,1] + fold_cm[0,1] > 0:\n",
    "        precision = fold_cm[1,1] / (fold_cm[1,1] + fold_cm[0,1])\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    if fold_cm[1,1] + fold_cm[1,0] > 0:\n",
    "        recall = fold_cm[1,1] / (fold_cm[1,1] + fold_cm[1,0])\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics\n",
    "aucs = [r['auc'] for r in fold_results]\n",
    "accs = [r['accuracy'] for r in fold_results]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS ACROSS FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean AUC:      {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "print(f\"Min AUC:       {np.min(aucs):.4f} (Fold {aucs.index(min(aucs))})\")\n",
    "print(f\"Max AUC:       {np.max(aucs):.4f} (Fold {aucs.index(max(aucs))})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d299c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-fold performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot AUC per fold\n",
    "folds = [r['fold'] for r in fold_results]\n",
    "aucs = [r['auc'] for r in fold_results]\n",
    "accs = [r['accuracy'] for r in fold_results]\n",
    "\n",
    "axes[0].bar(folds, aucs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=np.mean(aucs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(aucs):.4f}')\n",
    "axes[0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0].set_ylabel('AUC', fontsize=12)\n",
    "axes[0].set_title('AUC per Fold', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy per fold\n",
    "axes[1].bar(folds, accs, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=np.mean(accs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(accs):.4f}')\n",
    "axes[1].set_xlabel('Fold', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy per Fold', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Per-fold performance analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9121c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-fold performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot AUC per fold\n",
    "folds = [r['fold'] for r in fold_results]\n",
    "aucs = [r['auc'] for r in fold_results]\n",
    "accs = [r['accuracy'] for r in fold_results]\n",
    "\n",
    "axes[0].bar(folds, aucs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=np.mean(aucs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(aucs):.4f}')\n",
    "axes[0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0].set_ylabel('AUC', fontsize=12)\n",
    "axes[0].set_title('AUC per Fold', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy per fold\n",
    "axes[1].bar(folds, accs, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=np.mean(accs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(accs):.4f}')\n",
    "axes[1].set_xlabel('Fold', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy per Fold', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Per-fold performance analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on each fold separately\n",
    "print(\"\\n=== Per-Fold Performance Evaluation ===\\n\")\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold in sorted(cleaned_mapping['fold'].unique()):\n",
    "    # Get indices for this fold\n",
    "    fold_mask = cleaned_mapping['fold'] == fold\n",
    "    idx_fold = cleaned_mapping.index[fold_mask].to_numpy()\n",
    "    \n",
    "    # Create dataset and loader for this fold\n",
    "    fold_ds = ECGDataset(cleaned_mapping, idx_fold, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                         channel_mean=global_mean, channel_std=global_std)\n",
    "    fold_loader = DataLoader(fold_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Evaluate on this fold\n",
    "    model.eval()\n",
    "    fold_preds, fold_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in fold_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data).squeeze()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            fold_preds.extend(probs)\n",
    "            fold_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    fold_preds = np.array(fold_preds)\n",
    "    fold_targets = np.array(fold_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_auc = roc_auc_score(fold_targets, fold_preds) if len(np.unique(fold_targets)) > 1 else 0.5\n",
    "    fold_pred_labels = (fold_preds > 0.5).astype(int)\n",
    "    fold_acc = accuracy_score(fold_targets, fold_pred_labels)\n",
    "    fold_cm = confusion_matrix(fold_targets, fold_pred_labels)\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'n_samples': len(fold_targets),\n",
    "        'auc': fold_auc,\n",
    "        'accuracy': fold_acc,\n",
    "        'confusion_matrix': fold_cm\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"  Samples: {len(fold_targets):4d}\")\n",
    "    print(f\"  AUC:     {fold_auc:.4f}\")\n",
    "    print(f\"  Accuracy: {fold_acc:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\")\n",
    "    print(f\"    TN={fold_cm[0,0]:3d}  FP={fold_cm[0,1]:3d}\")\n",
    "    print(f\"    FN={fold_cm[1,0]:3d}  TP={fold_cm[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    if fold_cm[1,1] + fold_cm[0,1] > 0:\n",
    "        precision = fold_cm[1,1] / (fold_cm[1,1] + fold_cm[0,1])\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    if fold_cm[1,1] + fold_cm[1,0] > 0:\n",
    "        recall = fold_cm[1,1] / (fold_cm[1,1] + fold_cm[1,0])\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics\n",
    "aucs = [r['auc'] for r in fold_results]\n",
    "accs = [r['accuracy'] for r in fold_results]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS ACROSS FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean AUC:      {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "print(f\"Min AUC:       {np.min(aucs):.4f} (Fold {aucs.index(min(aucs))})\")\n",
    "print(f\"Max AUC:       {np.max(aucs):.4f} (Fold {aucs.index(max(aucs))})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no patient leakage between folds\n",
    "print(\"=== Verifying Patient-Level Fold Assignment ===\\n\")\n",
    "\n",
    "patient_fold_counts = cleaned_mapping.groupby('patient_id')['fold'].nunique()\n",
    "patients_in_multiple_folds = patient_fold_counts[patient_fold_counts > 1]\n",
    "\n",
    "print(f\"Total unique patients: {cleaned_mapping['patient_id'].nunique():,}\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"Patients appearing in multiple folds: {len(patients_in_multiple_folds)}\")\n",
    "\n",
    "if len(patients_in_multiple_folds) > 0:\n",
    "    print(\"\\n⚠️ WARNING: Patient leakage detected!\")\n",
    "    print(\"\\nPatients in multiple folds:\")\n",
    "    for patient, count in patients_in_multiple_folds.items():\n",
    "        folds = cleaned_mapping[cleaned_mapping['patient_id'] == patient]['fold'].unique()\n",
    "        print(f\"  {patient}: appears in {count} folds -> {sorted(folds)}\")\n",
    "else:\n",
    "    print(\"\\n✅ NO PATIENT LEAKAGE DETECTED\")\n",
    "    print(\"   Each patient appears in exactly one fold - proper stratification confirmed!\")\n",
    "\n",
    "# Distribution across folds\n",
    "print(\"\\n=== Fold Distribution Summary ===\")\n",
    "for fold in sorted(cleaned_mapping['fold'].unique()):\n",
    "    fold_data = cleaned_mapping[cleaned_mapping['fold'] == fold]\n",
    "    n_records = len(fold_data)\n",
    "    n_patients = fold_data['patient_id'].nunique()\n",
    "    n_afib = fold_data['_AFIB'].sum()\n",
    "    n_sr = (fold_data['_AFIB'] == 0).sum()\n",
    "    print(f\"Fold {fold}: {n_records:4d} records ({n_patients:4d} patients) | AFIB: {n_afib:3d} ({n_afib/n_records*100:.1f}%) | SR: {n_sr:3d} ({n_sr/n_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6c22a",
   "metadata": {},
   "source": [
    "## 18. Per-Fold Performance Analysis\n",
    "\n",
    "Evaluate model performance on each individual fold to ensure no patient leakage and assess generalization across different patient groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
