{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66c80b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PyTorch 2.10.0+cpu\n",
      "âœ“ scikit-learn 1.8.0\n",
      "âœ“ NumPy 2.4.2\n",
      "âœ“ Pandas 3.0.0\n",
      "âœ“ Matplotlib 3.10.8\n"
     ]
    }
   ],
   "source": [
    "# Verify required packages and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(f\"âœ“ PyTorch {torch.__version__}\")\n",
    "print(f\"âœ“ scikit-learn {sklearn.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "print(f\"âœ“ Pandas {pd.__version__}\")\n",
    "print(f\"âœ“ Matplotlib {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1b674",
   "metadata": {},
   "source": [
    "# Calibration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4bb1c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Calibration curves (reliability diagrams) assess how well model confidence aligns with true accuracy. A perfectly calibrated model's curve follows the diagonal (y=x).\n",
    "\n",
    "### Why Calibration Matters for AFib Detection\n",
    "\n",
    "In clinical practice, AFib detection models produce probability scores that physicians use to make decisions:\n",
    "- A score of **0.8 should mean ~80% likelihood of AFib** â€“ not just a classification threshold\n",
    "- **Overconfident models** (predictions > true frequency):\n",
    "  - Risk: Unnecessary referrals, excessive monitoring, patient anxiety\n",
    "  - Clinical consequence: Cost and burden increase without improved outcomes\n",
    "- **Underconfident models** (predictions < true frequency):\n",
    "  - Risk: Missed AFib cases, delayed treatment\n",
    "  - Clinical consequence: Potential serious cardiac events if AF goes undetected\n",
    "\n",
    "### What the Calibration Curve Shows\n",
    "\n",
    "- **Curve above diagonal**: Model is **under-confident** (predicted probabilities < actual outcome frequency)\n",
    "- **Curve below diagonal**: Model is **over-confident** (predicted probabilities > actual outcome frequency)\n",
    "- **Curve on diagonal**: **Perfect calibration** (ideal scenario)\n",
    "- **Brier Score**: Mean squared error between predicted probability and true label (0=perfect, 1=worst)\n",
    "\n",
    "### Resolution Analysis\n",
    "\n",
    "We examine 4 sampling rates (62.5Hz, 125Hz, 250Hz, 500Hz) because:\n",
    "- **Lower frequencies (62.5Hz)**: Minimal storage, fast inference, clinically sufficient for AFib detection\n",
    "- **Higher frequencies (500Hz)**: More signal detail, potential improved accuracy but increased complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699c2b1",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0175e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Notebook is here: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\adrian-testing\\calibration_curves\n",
      "ğŸ¯ Project root: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\n",
      "âœ… Checkpoint path: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl -> True\n",
      "âœ… Data path: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data -> True\n",
      "âœ“ Resolution folders in checkpoints: ['125hz', '250hz', '500hz', '62hz']\n",
      "âœ“ Model folders: ['cnn1d', 'cnn_lstm']\n",
      "âœ“ Data resolution folders: ['100hz', '125hz', '250hz', '500hz', '62hz']\n",
      "ğŸ–¥ï¸ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Project structure: AF_detection_vs_freq (3 levels up)\n",
    "PROJECT_ROOT = Path.cwd().parent.parent.parent\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"experiment_testing\" / \"mz-testing\" / \"checkpoints\" / \"ptbxl\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\"  # âœ… FIXED: Points to /data folder with resolution subfolders\n",
    "\n",
    "print(f\"ğŸ“ Notebook is here: {Path.cwd()}\")\n",
    "print(f\"ğŸ¯ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Checkpoint path: {CHECKPOINT_PATH} -> {CHECKPOINT_PATH.exists()}\")\n",
    "print(f\"âœ… Data path: {DATA_PATH} -> {DATA_PATH.exists()}\")\n",
    "\n",
    "# Verify folders\n",
    "res_folders = sorted([p.name for p in CHECKPOINT_PATH.glob(\"*hz\")]) if CHECKPOINT_PATH.exists() else []\n",
    "model_folders = sorted([p.name for p in (CHECKPOINT_PATH / \"62hz\").glob(\"*\")]) if (CHECKPOINT_PATH / \"62hz\").exists() else []\n",
    "data_folders = sorted([p.name for p in DATA_PATH.glob(\"*hz\")]) if DATA_PATH.exists() else []\n",
    "print(f\"âœ“ Resolution folders in checkpoints: {res_folders}\")\n",
    "print(f\"âœ“ Model folders: {model_folders}\")\n",
    "print(f\"âœ“ Data resolution folders: {data_folders}\")\n",
    "\n",
    "# Resolution and Model mappings\n",
    "RESOLUTIONS = {\n",
    "    \"62hz\": \"62.5Hz\",\n",
    "    \"125hz\": \"125Hz\",\n",
    "    \"250hz\": \"250Hz\",\n",
    "    \"500hz\": \"500Hz\"\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    \"cnn1d\": \"1D-CNN\",              # âœ… FIXED: cnn1d not cnn\n",
    "    \"cnn_lstm\": \"Hybrid CNN+LSTM\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80911a",
   "metadata": {},
   "source": [
    "## ğŸ“ File Structure & Paths\n",
    "\n",
    "**Checkpoints** (will be checked):\n",
    "- `experiment_testing/mz-testing/checkpoints/ptbxl/{62hz,125hz,250hz,500hz}/{cnn1d,cnn_lstm}/fold_1/best.pt`\n",
    "\n",
    "**Test Labels** (will be checked):\n",
    "- `prepared_data/{62hz,125hz,250hz,500hz}/test.pt` (preferred)\n",
    "- `prepared_data/{62hz,125hz,250hz,500hz}/sample_*.csv` (fallback)\n",
    "\n",
    "**Output** (will be generated):\n",
    "- 8 PNG files: `calibration_{model}_{resolution}.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "282f2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_test_labels(res_folder):\n",
    "    \"\"\"Load test labels from prepared_data/{res_folder}/best.pt\"\"\"\n",
    "    res_display = RESOLUTIONS.get(res_folder, res_folder)\n",
    "    data_dir = DATA_PATH / res_folder\n",
    "    \n",
    "    print(f\"    ğŸ“‚ Checking: {data_dir}\")\n",
    "    \n",
    "    # Try best.pt\n",
    "    test_pt_path = data_dir / \"best.pt\"\n",
    "    print(f\"    ğŸ” Trying: {test_pt_path.name}\")\n",
    "    if test_pt_path.exists():\n",
    "        try:\n",
    "            test_data = torch.load(test_pt_path, map_location=device)\n",
    "            if isinstance(test_data, dict) and 'labels' in test_data:\n",
    "                y_true = test_data['labels']\n",
    "                if isinstance(y_true, torch.Tensor):\n",
    "                    y_true = y_true.cpu().numpy()\n",
    "                print(f\"    âœ“ Loaded labels from best.pt ({len(y_true)} samples)\")\n",
    "                return y_true\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Error loading best.pt: {e}\")\n",
    "    else:\n",
    "        print(f\"    âœ— best.pt not found at {test_pt_path}\")\n",
    "    \n",
    "\n",
    "def load_checkpoint_and_predict(model_folder, res_folder):\n",
    "    \"\"\"Load checkpoint and extract predictions.\"\"\"\n",
    "    checkpoint_path = CHECKPOINT_PATH / res_folder / model_folder / \"fold_1\" / \"best.pt\"\n",
    "    print(f\"    ğŸ” Checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"    âœ— NOT FOUND: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        checkpoint_keys = list(checkpoint.keys()) if isinstance(checkpoint, dict) else type(checkpoint).__name__\n",
    "        print(f\"    âœ“ Loaded checkpoint (type: {type(checkpoint).__name__})\")\n",
    "        \n",
    "        if isinstance(checkpoint, dict) and 'predictions' in checkpoint:\n",
    "            y_probs = checkpoint['predictions']\n",
    "            if isinstance(y_probs, torch.Tensor):\n",
    "                y_probs = y_probs.cpu().numpy()\n",
    "            print(f\"    âœ“ Extracted predictions ({len(y_probs)} samples)\")\n",
    "            return y_probs\n",
    "        \n",
    "        elif isinstance(checkpoint, dict) and ('model_state_dict' in checkpoint or 'state_dict' in checkpoint):\n",
    "            print(f\"    â„¹ State dict detected (model instantiation needed)\")\n",
    "            return None\n",
    "        \n",
    "        elif isinstance(checkpoint, (torch.Tensor, np.ndarray)):\n",
    "            y_probs = checkpoint\n",
    "            if isinstance(y_probs, torch.Tensor):\n",
    "                y_probs = y_probs.cpu().numpy()\n",
    "            print(f\"    âœ“ Extracted predictions ({len(y_probs)} samples)\")\n",
    "            return y_probs\n",
    "        \n",
    "        else:\n",
    "            print(f\"    âš  Unknown checkpoint structure: {type(checkpoint)}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_calibration_data(res_folder, model_folder):\n",
    "    \"\"\"Get y_true and y_probs with detailed debug output.\"\"\"\n",
    "    print(f\"\\n  ğŸ“Š {MODELS[model_folder]} @ {RESOLUTIONS[res_folder]}\")\n",
    "    print(f\"  {'='*70}\")\n",
    "    \n",
    "    # Load test labels\n",
    "    print(f\"  [1/2] Loading test labels...\")\n",
    "    y_true = load_test_labels(res_folder)\n",
    "    if y_true is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Load predictions from checkpoint\n",
    "    print(f\"  [2/2] Loading checkpoint predictions...\")\n",
    "    y_probs = load_checkpoint_and_predict(model_folder, res_folder)\n",
    "    if y_probs is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Validate shapes match\n",
    "    if len(y_true) != len(y_probs):\n",
    "        print(f\"  âœ— Shape mismatch: y_true ({len(y_true)}) vs y_probs ({len(y_probs)})\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"  âœ“ Loaded {len(y_true)} samples\")\n",
    "    return y_true, y_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20a102",
   "metadata": {},
   "source": [
    "## Model Architecture Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6238765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model architectures loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define CNN1D Architecture (from mz-testing/models/cnn1d.py)\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# Define CNN+LSTM Architecture (from mz-testing/models/cnn_lstm.py)\n",
    "class ShortcutConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=10, dropout=0.3, pool_kernel=2):\n",
    "        super().__init__()\n",
    "        if out_ch % 2 != 0:\n",
    "            raise ValueError(\"out_ch must be even\")\n",
    "        self.bn = nn.BatchNorm1d(in_ch)\n",
    "        self.conv = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, \n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=pool_kernel, stride=pool_kernel)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=pool_kernel, stride=pool_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        c_half = x.size(1) // 2\n",
    "        x_main = x[:, :c_half, :]\n",
    "        x_short = x[:, c_half:, :]\n",
    "        x_main = self.avgpool(x_main)\n",
    "        x_short = self.maxpool(x_short)\n",
    "        return torch.cat([x_main, x_short], dim=1)\n",
    "\n",
    "\n",
    "class CNN_LSTM_ECG(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        ch = [32, 32, 64, 64, 128, 128, 256, 256]\n",
    "        blocks = []\n",
    "        c_in = in_channels\n",
    "        for c_out in ch:\n",
    "            blocks.append(ShortcutConvBlock1D(in_ch=c_in, out_ch=c_out, \n",
    "                                             kernel_size=10, dropout=dropout, pool_kernel=2))\n",
    "            c_in = c_out\n",
    "        self.cnn = nn.Sequential(*blocks)\n",
    "        self.lstm = nn.LSTM(input_size=ch[-1], hidden_size=128, \n",
    "                           num_layers=1, batch_first=True, dropout=0.0, bidirectional=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out.mean(dim=1)\n",
    "        return self.classifier(out)\n",
    "\n",
    "print(\"âœ… Model architectures loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ca8aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_calibration_curve(y_true, y_probs, model_name, resolution, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Generate and save a calibration curve PNG.\n",
    "    \n",
    "    Args:\n",
    "        y_true: np.array of true binary labels [n_samples]\n",
    "        y_probs: np.array of predicted AFib probabilities [n_samples], range [0, 1]\n",
    "        model_name: str, display name e.g., \"1D-CNN\"\n",
    "        resolution: str, display name e.g., \"62.5Hz\"\n",
    "        output_dir: directory to save PNG\n",
    "    \n",
    "    Returns:\n",
    "        dict with metrics: {model, resolution, brier, n_samples, mean_pred, mean_true, filepath}\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure probabilities are in valid range\n",
    "    y_probs = np.clip(y_probs, 0, 1)\n",
    "    \n",
    "    # Compute Brier score\n",
    "    brier = brier_score_loss(y_true, y_probs)\n",
    "    \n",
    "    # Compute statistics\n",
    "    n_samples = len(y_true)\n",
    "    mean_pred = np.mean(y_probs)\n",
    "    mean_true = np.mean(y_true)\n",
    "    \n",
    "    # Warn if small sample size\n",
    "    if n_samples < 50:\n",
    "        print(f\"  âš  Warning: Small sample size ({n_samples} < 50)\")\n",
    "    \n",
    "    print(f\"  Brier Score: {brier:.6f}\")\n",
    "    print(f\"  Mean Pred: {mean_pred:.4f} | Mean True: {mean_true:.4f} | N={n_samples}\")\n",
    "    print(f\"  Class dist: {np.sum(y_true==0)} normal, {np.sum(y_true==1)} AFib\")\n",
    "    \n",
    "    # Create figure with calibration curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Generate calibration curve using sklearn\n",
    "    display = CalibrationDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_probs,\n",
    "        n_bins=10,\n",
    "        strategy=\"uniform\",\n",
    "        ax=ax,\n",
    "        color=\"steelblue\",\n",
    "        name=\"Model\"\n",
    "    )\n",
    "    \n",
    "    # Note: CalibrationDisplay already adds the perfect calibration diagonal\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(f\"Calibration Curve â€“ {model_name}, {resolution}\", \n",
    "                 fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Mean Predicted Probability (AFib)\", fontsize=12)\n",
    "    ax.set_ylabel(\"True Positive Fraction (AFib)\", fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"lower right\", fontsize=10)\n",
    "    \n",
    "    # Add Brier score annotation\n",
    "    textstr = f\"Brier Score: {brier:.6f}\"\n",
    "    ax.text(0.98, 0.02, textstr, transform=ax.transAxes, \n",
    "            fontsize=10, verticalalignment='bottom', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure (safe filename)\n",
    "    model_safe = model_name.replace(\"+\", \"\").replace(\" \", \"_\")\n",
    "    res_safe = resolution.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    output_filename = f\"calibration_{model_safe}_{res_safe}.png\"\n",
    "    output_path = output_dir / output_filename\n",
    "    \n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  âœ“ Saved: {output_filename}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Resolution\": resolution,\n",
    "        \"Brier Score\": brier,\n",
    "        \"N Samples\": n_samples,\n",
    "        \"Mean Pred Prob\": mean_pred,\n",
    "        \"Mean True Label\": mean_true,\n",
    "        \"Filepath\": str(output_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aca85e",
   "metadata": {},
   "source": [
    "## Generate Individual Calibration Curves for All Model-Resolution Combinations\n",
    "This cell generates calibration curves for each of the 12 combinations (3 models Ã— 4 resolutions).\n",
    "Each curve is saved as a standalone PNG file with clear labels and perfect diagonal reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ac3deaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Loading test data files from /data folder...\n",
      "  âœ… Found 62hz: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\62hz\\test\\test.pt\n",
      "  âœ… Found 125hz: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\125hz\\test\\test.pt\n",
      "  âœ… Found 250hz: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\250hz\\test\\test.pt\n",
      "  âœ… Found 500hz: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\500hz\\test\\test.pt\n",
      "\n",
      "ğŸ“¦ Total test data files found: 4\n",
      "  62.5Hz: True\n",
      "  125Hz: True\n",
      "  250Hz: True\n",
      "  500Hz: True\n"
     ]
    }
   ],
   "source": [
    "# Define test data paths directly based on known structure\n",
    "def find_test_data_files():\n",
    "    \"\"\"Get test data files from data/{resolution}/test/test.pt\"\"\"\n",
    "    test_files = {}\n",
    "    project_root = Path.cwd().parent.parent.parent\n",
    "    data_root = project_root / \"data\"\n",
    "    \n",
    "    print(\"ğŸ” Loading test data files from /data folder...\")\n",
    "    \n",
    "    for res in RESOLUTIONS.keys():\n",
    "        test_path = data_root / res / \"test\" / \"test.pt\"\n",
    "        if test_path.exists():\n",
    "            test_files[res] = test_path\n",
    "            print(f\"  âœ… Found {res}: {test_path}\")\n",
    "        else:\n",
    "            print(f\"  âŒ Missing {res}: {test_path}\")\n",
    "    \n",
    "    return test_files\n",
    "\n",
    "\n",
    "# Find test data\n",
    "test_data_files = find_test_data_files()\n",
    "print(f\"\\nğŸ“¦ Total test data files found: {len(test_data_files)}\")\n",
    "for res, path in test_data_files.items():\n",
    "    print(f\"  {RESOLUTIONS[res]}: {path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816b600",
   "metadata": {},
   "source": [
    "## STEP 3: Main Loop Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca2c26",
   "metadata": {},
   "source": [
    "## STEP 1: Inspect Test Data Structure\n",
    "\n",
    "Before loading all test data, let's inspect one file to understand its structure (keys, shapes, types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5562f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Inspecting test data structure...\n",
      "================================================================================\n",
      "Loading: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\125hz\\test\\test.pt\n",
      "Exists: True\n",
      "\n",
      "Type: <class 'dict'>\n",
      "Keys: ['X', 'y', 'record_ids', 'patient_ids']\n",
      "\n",
      "Details:\n",
      "  'X':\n",
      "    - type: Tensor\n",
      "    - shape: torch.Size([2185, 12, 1250])\n",
      "    - length: 2185\n",
      "    - dtype: torch.float32\n",
      "  'y':\n",
      "    - type: Tensor\n",
      "    - shape: torch.Size([2185])\n",
      "    - length: 2185\n",
      "    - dtype: torch.int64\n",
      "  'record_ids':\n",
      "    - type: list\n",
      "    - shape: None\n",
      "    - length: 2185\n",
      "    - dtype: None\n",
      "  'patient_ids':\n",
      "    - type: list\n",
      "    - shape: None\n",
      "    - length: 2185\n",
      "    - dtype: None\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Inspect one test file to see its keys and shapes\n",
    "print(\"ğŸ” Inspecting test data structure...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to load a sample test file (125hz as example)\n",
    "sample_test_path = PROJECT_ROOT / \"data\" / \"125hz\" / \"test\" / \"test.pt\"\n",
    "if not sample_test_path.exists():\n",
    "    # Try 62hz instead\n",
    "    sample_test_path = PROJECT_ROOT / \"data\" / \"62hz\" / \"test\" / \"test.pt\"\n",
    "\n",
    "print(f\"Loading: {sample_test_path}\")\n",
    "print(f\"Exists: {sample_test_path.exists()}\")\n",
    "\n",
    "if sample_test_path.exists():\n",
    "    test_data = torch.load(sample_test_path, map_location=\"cpu\")\n",
    "    print(f\"\\nType: {type(test_data)}\")\n",
    "    \n",
    "    if isinstance(test_data, dict):\n",
    "        print(f\"Keys: {list(test_data.keys())}\")\n",
    "        print(\"\\nDetails:\")\n",
    "        for k, v in test_data.items():\n",
    "            try:\n",
    "                v_type = type(v).__name__\n",
    "                v_shape = getattr(v, 'shape', None)\n",
    "                v_len = len(v) if hasattr(v, '__len__') else 'n/a'\n",
    "                v_dtype = getattr(v, 'dtype', None)\n",
    "                print(f\"  '{k}':\")\n",
    "                print(f\"    - type: {v_type}\")\n",
    "                print(f\"    - shape: {v_shape}\")\n",
    "                print(f\"    - length: {v_len}\")\n",
    "                print(f\"    - dtype: {v_dtype}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  '{k}': type={type(v)}, error={e}\")\n",
    "    else:\n",
    "        print(\"âš  Non-dict structure\")\n",
    "        print(f\"Repr: {repr(test_data)[:300]}\")\n",
    "else:\n",
    "    print(\"âŒ Test file not found!\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af025cc1",
   "metadata": {},
   "source": [
    "## Helper Function: Extract Features and Labels\n",
    "\n",
    "This function explicitly checks for keys instead of using `or` operator, which causes RuntimeError with Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d3ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function loaded: extract_xy()\n"
     ]
    }
   ],
   "source": [
    "def extract_xy(test_data):\n",
    "    \"\"\"\n",
    "    Extract features (X) and labels (y) from test data dictionary.\n",
    "    \n",
    "    âš  CRITICAL: Use explicit key checks, NOT 'or' operator with Tensors!\n",
    "    The pattern `tensor_a or tensor_b` raises: \"Boolean value of Tensor with more than one value is ambiguous\"\n",
    "    \n",
    "    Args:\n",
    "        test_data: dict with test data\n",
    "        \n",
    "    Returns:\n",
    "        tuple (X, y_true) as Tensors\n",
    "    \"\"\"\n",
    "    if not isinstance(test_data, dict):\n",
    "        raise TypeError(f\"Expected dict, got {type(test_data)}\")\n",
    "    \n",
    "    # Extract labels with explicit key checks\n",
    "    if \"y\" in test_data:\n",
    "        y_true = test_data[\"y\"]\n",
    "    elif \"labels\" in test_data:\n",
    "        y_true = test_data[\"labels\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No label key found. Available keys: {list(test_data.keys())}\")\n",
    "    \n",
    "    # Extract features with explicit key checks\n",
    "    if \"X\" in test_data:\n",
    "        X = test_data[\"X\"]\n",
    "    elif \"x\" in test_data:\n",
    "        X = test_data[\"x\"]\n",
    "    elif \"signals\" in test_data:\n",
    "        X = test_data[\"signals\"]\n",
    "    elif \"data\" in test_data:\n",
    "        X = test_data[\"data\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No feature key found. Available keys: {list(test_data.keys())}\")\n",
    "    \n",
    "    return X, y_true\n",
    "\n",
    "print(\"âœ… Helper function loaded: extract_xy()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11435f5",
   "metadata": {},
   "source": [
    "## STEP 2: Load Models & Generate Calibration Curves\n",
    "\n",
    "### âš  Bug Fix: RuntimeError with Tensor boolean evaluation\n",
    "\n",
    "**Problem:**\n",
    "```python\n",
    "# âŒ BUGGY CODE:\n",
    "y_true = test_data.get('y') or test_data.get('labels')\n",
    "```\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n",
    "```\n",
    "\n",
    "**Root Cause:**\n",
    "- Python's `or` operator evaluates the left operand first: `A or B`\n",
    "- If `A` is a non-empty Tensor, Python tries to convert it to boolean\n",
    "- Multi-element Tensors cannot be used in boolean context â†’ RuntimeError\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# âœ… FIXED CODE:\n",
    "if \"y\" in test_data:\n",
    "    y_true = test_data[\"y\"]\n",
    "elif \"labels\" in test_data:\n",
    "    y_true = test_data[\"labels\"]\n",
    "else:\n",
    "    raise KeyError(...)\n",
    "```\n",
    "\n",
    "We use the `extract_xy()` helper function that implements explicit key checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7d19532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸš€ MAIN LOOP: Load Models + Run Inference + Generate Calibration Curves\n",
      "====================================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š 1D-CNN @ 62.5Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\62hz\\cnn1d\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\62hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 620]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0000, 1.0000]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.010784\n",
      "  Mean Pred: 0.1429 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_1D-CNN_625Hz.png\n",
      "      âœ… SUCCESS: calibration_1D-CNN_625Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š Hybrid CNN+LSTM @ 62.5Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\62hz\\cnn_lstm\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\62hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 620]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0025, 0.9988]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.015501\n",
      "  Mean Pred: 0.1447 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_Hybrid_CNNLSTM_625Hz.png\n",
      "      âœ… SUCCESS: calibration_Hybrid_CNNLSTM_625Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š 1D-CNN @ 125Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\125hz\\cnn1d\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\125hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 1250]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0000, 1.0000]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.013038\n",
      "  Mean Pred: 0.1456 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_1D-CNN_125Hz.png\n",
      "      âœ… SUCCESS: calibration_1D-CNN_125Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š Hybrid CNN+LSTM @ 125Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\125hz\\cnn_lstm\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\125hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 1250]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0012, 0.9994]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.012609\n",
      "  Mean Pred: 0.1448 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_Hybrid_CNNLSTM_125Hz.png\n",
      "      âœ… SUCCESS: calibration_Hybrid_CNNLSTM_125Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š 1D-CNN @ 250Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\250hz\\cnn1d\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\250hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 2500]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0000, 1.0000]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.013949\n",
      "  Mean Pred: 0.1455 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_1D-CNN_250Hz.png\n",
      "      âœ… SUCCESS: calibration_1D-CNN_250Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š Hybrid CNN+LSTM @ 250Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\250hz\\cnn_lstm\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\250hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 2500]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0010, 0.9997]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.016043\n",
      "  Mean Pred: 0.1441 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_Hybrid_CNNLSTM_250Hz.png\n",
      "      âœ… SUCCESS: calibration_Hybrid_CNNLSTM_250Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š 1D-CNN @ 500Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\500hz\\cnn1d\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\500hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 5000]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0000, 1.0000]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.028855\n",
      "  Mean Pred: 0.1648 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_1D-CNN_500Hz.png\n",
      "      âœ… SUCCESS: calibration_1D-CNN_500Hz.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š Hybrid CNN+LSTM @ 500Hz\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/3] Checkpoint: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\experiment_testing\\mz-testing\\checkpoints\\ptbxl\\500hz\\cnn_lstm\\fold_1\\best.pt\n",
      "      Exists: True\n",
      "[2/3] Test Data: c:\\Users\\adria\\VSCode_Projects\\bachelorWork\\AF_detection_vs_freq\\data\\500hz\\test\\test.pt\n",
      "      Exists: True\n",
      "[2b] Loading test data...\n",
      "      âœ“ Loaded 2185 test samples\n",
      "      X shape: torch.Size([2185, 12, 5000]), y shape: (2185,)\n",
      "      y range: [0.00, 1.00]\n",
      "[3/3] Loading model and running inference...\n",
      "      âœ“ Model loaded\n",
      "      âœ“ Inference complete: 2185 predictions\n",
      "      Prob range: [0.0001, 0.9999]\n",
      "      Label distribution: 1894 normal, 291 AFib\n",
      "      ğŸ¨ Generating calibration curve...\n",
      "  Brier Score: 0.015197\n",
      "  Mean Pred: 0.1521 | Mean True: 0.1332 | N=2185\n",
      "  Class dist: 1894 normal, 291 AFib\n",
      "  âœ“ Saved: calibration_Hybrid_CNNLSTM_500Hz.png\n",
      "      âœ… SUCCESS: calibration_Hybrid_CNNLSTM_500Hz.png\n",
      "\n",
      "====================================================================================================\n",
      "âœ… COMPLETED: Generated 8 calibration curves\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize results storage\n",
    "results_list = []\n",
    "saved_files = []\n",
    "\n",
    "# Storage for 2x2 comparison plots (collected by resolution and model)\n",
    "y_true_by_res = {}  # {res_folder: y_true_array}\n",
    "y_probs_by_res_model = {}  # {res_folder: {model_folder: y_probs_array}}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸš€ MAIN LOOP: Load Models + Run Inference + Generate Calibration Curves\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Loop through all model-resolution combinations\n",
    "for res_folder, res_display in RESOLUTIONS.items():\n",
    "    for model_folder, model_display in MODELS.items():\n",
    "        \n",
    "        print(f\"\\n{'â”€'*100}\")\n",
    "        print(f\"ğŸ“Š {model_display} @ {res_display}\")\n",
    "        print(f\"{'â”€'*100}\")\n",
    "        \n",
    "        # Get checkpoint path\n",
    "        checkpoint_path = CHECKPOINT_PATH / res_folder / model_folder / \"fold_1\" / \"best.pt\"\n",
    "        print(f\"[1/3] Checkpoint: {checkpoint_path}\")\n",
    "        print(f\"      Exists: {checkpoint_path.exists()}\")\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"      âŒ SKIPPED: Checkpoint not found\")\n",
    "            continue\n",
    "        \n",
    "        # Get test data path\n",
    "        test_data_path = test_data_files.get(res_folder)\n",
    "        if test_data_path is None:\n",
    "            print(f\"[2/3] Test Data: âŒ NOT FOUND for {res_folder}\")\n",
    "            continue\n",
    "        print(f\"[2/3] Test Data: {test_data_path}\")\n",
    "        print(f\"      Exists: {test_data_path.exists()}\")\n",
    "        \n",
    "        try:\n",
    "            # Load test data\n",
    "            print(f\"[2b] Loading test data...\")\n",
    "            test_data = torch.load(test_data_path, map_location='cpu')\n",
    "            \n",
    "            # âœ… FIXED: Use explicit key checks instead of 'or' operator\n",
    "            # OLD (BUGGY): y_true = test_data.get('y') or test_data.get('labels')\n",
    "            # NEW: Use helper function with explicit key checks\n",
    "            X_test, y_true = extract_xy(test_data)\n",
    "            \n",
    "            # Convert to proper format\n",
    "            if isinstance(y_true, torch.Tensor):\n",
    "                y_true = y_true.detach().cpu().numpy().ravel()\n",
    "            else:\n",
    "                y_true = np.array(y_true).ravel()\n",
    "            \n",
    "            if isinstance(X_test, torch.Tensor):\n",
    "                X_test = X_test.detach().cpu()\n",
    "            else:\n",
    "                X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "            \n",
    "            print(f\"      âœ“ Loaded {len(y_true)} test samples\")\n",
    "            print(f\"      X shape: {X_test.shape}, y shape: {y_true.shape}\")\n",
    "            print(f\"      y range: [{y_true.min():.2f}, {y_true.max():.2f}]\")\n",
    "            \n",
    "            # Load checkpoint and create model\n",
    "            print(f\"[3/3] Loading model and running inference...\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            # Initialize appropriate model\n",
    "            if model_folder == \"cnn1d\":\n",
    "                model = CNN1D(in_channels=12, num_classes=2)\n",
    "            elif model_folder == \"cnn_lstm\":\n",
    "                model = CNN_LSTM_ECG(in_channels=12, num_classes=2)\n",
    "            else:\n",
    "                print(f\"      âŒ Unknown model: {model_folder}\")\n",
    "                continue\n",
    "            \n",
    "            # Load state dict\n",
    "            model.load_state_dict(checkpoint)\n",
    "            model.eval()\n",
    "            model = model.to('cpu')\n",
    "            \n",
    "            print(f\"      âœ“ Model loaded\")\n",
    "            \n",
    "            # Run inference to get predictions\n",
    "            with torch.no_grad():\n",
    "                batch_size = 32\n",
    "                y_probs_list = []\n",
    "                \n",
    "                for i in range(0, len(X_test), batch_size):\n",
    "                    batch = X_test[i:i+batch_size]\n",
    "                    if not isinstance(batch, torch.Tensor):\n",
    "                        batch = torch.tensor(batch, dtype=torch.float32)\n",
    "                    batch = batch.to('cpu')\n",
    "                    \n",
    "                    logits = model(batch)  # (B, num_classes)\n",
    "                    \n",
    "                    # Get probability for AFib class (class 1)\n",
    "                    if logits.shape[-1] == 2:\n",
    "                        probs = torch.softmax(logits, dim=-1)[:, 1]  # AFib is class 1\n",
    "                    else:\n",
    "                        probs = torch.sigmoid(logits.squeeze())\n",
    "                    \n",
    "                    y_probs_list.append(probs.cpu().numpy())\n",
    "                \n",
    "                y_probs = np.concatenate(y_probs_list, axis=0)\n",
    "            \n",
    "            print(f\"      âœ“ Inference complete: {len(y_probs)} predictions\")\n",
    "            print(f\"      Prob range: [{np.min(y_probs):.4f}, {np.max(y_probs):.4f}]\")\n",
    "            \n",
    "            # Ensure labels are binary (0 or 1)\n",
    "            y_true_binary = (y_true > 0.5).astype(int) if y_true.dtype == np.float64 or y_true.dtype == np.float32 else y_true.astype(int)\n",
    "            \n",
    "            print(f\"      Label distribution: {np.sum(y_true_binary==0)} normal, {np.sum(y_true_binary==1)} AFib\")\n",
    "            \n",
    "            # Store data for comparison plots\n",
    "            if res_folder not in y_true_by_res:\n",
    "                y_true_by_res[res_folder] = y_true_binary\n",
    "            if res_folder not in y_probs_by_res_model:\n",
    "                y_probs_by_res_model[res_folder] = {}\n",
    "            y_probs_by_res_model[res_folder][model_folder] = y_probs\n",
    "            \n",
    "            # Generate calibration curve\n",
    "            print(f\"      ğŸ¨ Generating calibration curve...\")\n",
    "            result = generate_calibration_curve(y_true_binary, y_probs, model_display, res_display, output_dir=\".\")\n",
    "            results_list.append(result)\n",
    "            \n",
    "            # Track saved file\n",
    "            model_safe = model_display.replace(\"+\", \"\").replace(\" \", \"_\")\n",
    "            res_safe = res_display.replace(\".\", \"\").replace(\" \", \"\")\n",
    "            output_filename = f\"calibration_{model_safe}_{res_safe}.png\"\n",
    "            saved_files.append(output_filename)\n",
    "            \n",
    "            print(f\"      âœ… SUCCESS: {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"âœ… COMPLETED: Generated {len(saved_files)} calibration curves\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6562a44",
   "metadata": {},
   "source": [
    "## Generate 2Ã—2 Comparison Plots\n",
    "\n",
    "Create one plot per model showing all 4 resolutions side-by-side for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e7fe646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ¨ GENERATING 2Ã—2 COMPARISON PLOTS\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Creating comparison plot for 1D-CNN...\n",
      "  âœ… Saved: calibration_comparison_1D-CNN.png\n",
      "\n",
      "ğŸ“Š Creating comparison plot for Hybrid CNN+LSTM...\n",
      "  âœ… Saved: calibration_comparison_Hybrid_CNNLSTM.png\n",
      "\n",
      "====================================================================================================\n",
      "âœ… COMPLETED: Generated 2 comparison plots\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ¨ GENERATING 2Ã—2 COMPARISON PLOTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Generate one 2Ã—2 plot per model (all resolutions)\n",
    "comparison_files = []\n",
    "\n",
    "for model_folder, model_display in MODELS.items():\n",
    "    print(f\"\\nğŸ“Š Creating comparison plot for {model_display}...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (res_folder, res_display) in enumerate(RESOLUTIONS.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Check if we have data for this resolution and model\n",
    "        has_data = (res_folder in y_true_by_res and \n",
    "                    res_folder in y_probs_by_res_model and \n",
    "                    model_folder in y_probs_by_res_model.get(res_folder, {}))\n",
    "        \n",
    "        if not has_data:\n",
    "            ax.text(0.5, 0.5, f\"No data for {res_display}\", \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "            continue\n",
    "        \n",
    "        # Get data\n",
    "        y_true = y_true_by_res[res_folder]\n",
    "        y_probs = y_probs_by_res_model[res_folder][model_folder]\n",
    "        \n",
    "        # Compute metrics\n",
    "        brier = brier_score_loss(y_true, y_probs)\n",
    "        \n",
    "        # Generate calibration curve on this subplot\n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_probs,\n",
    "            n_bins=10,\n",
    "            strategy=\"uniform\",\n",
    "            ax=ax,\n",
    "            color=\"steelblue\",\n",
    "            name=None  # Don't add legend label\n",
    "        )\n",
    "        \n",
    "        # Customize subplot\n",
    "        ax.set_title(f\"{res_display} (n={len(y_true)})\", fontsize=13, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Mean Predicted Probability (AFib)\", fontsize=11)\n",
    "        ax.set_ylabel(\"True Positive Fraction (AFib)\", fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend().set_visible(False)  # Remove legend for cleaner look\n",
    "        \n",
    "        # Add Brier score annotation\n",
    "        ax.text(0.98, 0.02, f\"Brier: {brier:.4f}\", transform=ax.transAxes,\n",
    "                fontsize=10, verticalalignment='bottom', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Set overall title\n",
    "    fig.suptitle(f\"Calibration Curves â€“ {model_display} (All Resolutions)\", \n",
    "                 fontsize=16, fontweight=\"bold\", y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    model_safe = model_display.replace(\"+\", \"\").replace(\" \", \"_\")\n",
    "    output_filename = f\"calibration_comparison_{model_safe}.png\"\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    comparison_files.append(output_filename)\n",
    "    print(f\"  âœ… Saved: {output_filename}\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"âœ… COMPLETED: Generated {len(comparison_files)} comparison plots\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7d5fc",
   "metadata": {},
   "source": [
    "## Generate Overlay Comparison Plots\n",
    "\n",
    "Create one plot per model with all 4 resolutions overlaid as distinct colored lines for direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "199633b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ¨ GENERATING PUBLICATION-READY OVERLAY COMPARISON PLOTS\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Creating publication-ready overlay plot for 1D-CNN...\n",
      "  âœ“ Plotted 62.5Hz with Brier: 0.0108\n",
      "  âœ“ Plotted 125Hz with Brier: 0.0130\n",
      "  âœ“ Plotted 250Hz with Brier: 0.0139\n",
      "  âœ“ Plotted 500Hz with Brier: 0.0289\n",
      "  âœ… Saved: calibration_1dcnn.png\n",
      "\n",
      "ğŸ“Š Creating publication-ready overlay plot for Hybrid CNN+LSTM...\n",
      "  âœ“ Plotted 62.5Hz with Brier: 0.0155\n",
      "  âœ“ Plotted 125Hz with Brier: 0.0126\n",
      "  âœ“ Plotted 250Hz with Brier: 0.0160\n",
      "  âœ“ Plotted 500Hz with Brier: 0.0152\n",
      "  âœ… Saved: calibration_hybrid_cnnlstm.png\n",
      "\n",
      "====================================================================================================\n",
      "âœ… Individual plots completed: 2 files\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Creating combined figure* for IEEE paper...\n",
      "  âœ… Saved combined figure: fig_calibration.png\n",
      "\n",
      "====================================================================================================\n",
      "âœ… PUBLICATION-READY PLOTS GENERATED\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“„ LaTeX-ready filenames for \\includegraphics:\n",
      "--------------------------------------------------------------------------------\n",
      "fig_calibration.png          % figure* (both models side-by-side)\n",
      "calibration_1dcnn.png        % Individual 1DCNN\n",
      "calibration_hybrid_cnnlstm.png % Individual HYBRID\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Files generated:\n",
      "  â€¢ Combined figure: fig_calibration.png\n",
      "  â€¢ Individual plots: calibration_1dcnn.png, calibration_hybrid_cnnlstm.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ¨ GENERATING PUBLICATION-READY OVERLAY COMPARISON PLOTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Color gradient: bright yellow (62.5Hz) to dark red (500Hz)\n",
    "colors = {\n",
    "    \"62hz\": \"#FFFF00\",      # Bright yellow\n",
    "    \"125hz\": \"#FFA500\",     # Orange\n",
    "    \"250hz\": \"#FF6347\",     # Tomato/Red-orange\n",
    "    \"500hz\": \"#8B0000\"      # Dark red\n",
    "}\n",
    "\n",
    "overlay_files = []\n",
    "individual_axes = []  # Store axes for combined figure\n",
    "\n",
    "# Model name mapping for publication\n",
    "pub_model_names = {\n",
    "    \"cnn1d\": \"1D-CNN\",\n",
    "    \"cnn_lstm\": \"CNN-LSTM Hybrid\"\n",
    "}\n",
    "\n",
    "# STEP 1: Generate individual plots (IEEE quality)\n",
    "for model_folder, model_display in MODELS.items():\n",
    "    print(f\"\\nğŸ“Š Creating publication-ready overlay plot for {model_display}...\")\n",
    "    \n",
    "    # IEEE paper column width: 12x5 inches\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    for res_folder, res_display in RESOLUTIONS.items():\n",
    "        # Check if we have data\n",
    "        has_data = (res_folder in y_true_by_res and \n",
    "                    res_folder in y_probs_by_res_model and \n",
    "                    model_folder in y_probs_by_res_model.get(res_folder, {}))\n",
    "        \n",
    "        if not has_data:\n",
    "            print(f\"  âš  No data for {res_display}\")\n",
    "            continue\n",
    "        \n",
    "        # Get data\n",
    "        y_true = y_true_by_res[res_folder]\n",
    "        y_probs = y_probs_by_res_model[res_folder][model_folder]\n",
    "        \n",
    "        # Compute Brier score\n",
    "        brier = brier_score_loss(y_true, y_probs)\n",
    "        \n",
    "        # Plot calibration curve with custom color\n",
    "        color = colors.get(res_folder, \"blue\")\n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_probs,\n",
    "            n_bins=10,\n",
    "            strategy=\"uniform\",\n",
    "            ax=ax,\n",
    "            color=color,\n",
    "            name=f\"{res_display} (Brier: {brier:.4f})\",\n",
    "            linewidth=2.5,\n",
    "            marker='o',\n",
    "            markersize=6\n",
    "        )\n",
    "        \n",
    "        print(f\"  âœ“ Plotted {res_display} with Brier: {brier:.4f}\")\n",
    "    \n",
    "    # âœ… IEEE PUBLICATION FORMATTING\n",
    "    # Title with clinical context\n",
    "    pub_name = pub_model_names.get(model_folder, model_display)\n",
    "    ax.set_title(f\"Calibration Curves â€“ {pub_name} (AFib Positive Class)\", \n",
    "                 fontsize=14, fontweight=\"bold\", pad=15)\n",
    "    \n",
    "    # Precise axis labels\n",
    "    ax.set_xlabel(\"Mean Predicted Probability (AFib)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Fraction of Positives (AFib)\", fontsize=12)\n",
    "    \n",
    "    # Legend in upper left (less overlap)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(0.02, 0.98), \n",
    "             fontsize=10, framealpha=0.95, edgecolor='gray')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Perfect calibration diagonal\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=2, alpha=0.5, zorder=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with IEEE quality settings\n",
    "    model_safe = model_display.replace(\"+\", \"\").replace(\" \", \"_\").replace(\"-\", \"\").lower()\n",
    "    output_filename = f\"calibration_{model_safe}.png\"\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    overlay_files.append(output_filename)\n",
    "    print(f\"  âœ… Saved: {output_filename}\")\n",
    "    \n",
    "    # Store axis for combined figure\n",
    "    individual_axes.append((model_folder, model_display, ax))\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"âœ… Individual plots completed: {len(overlay_files)} files\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# STEP 2: Generate combined figure* (both models side-by-side)\n",
    "print(\"\\nğŸ“Š Creating combined figure* for IEEE paper...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes_list = [ax1, ax2]\n",
    "\n",
    "for idx, (model_folder, model_display) in enumerate(MODELS.items()):\n",
    "    ax = axes_list[idx]\n",
    "    \n",
    "    for res_folder, res_display in RESOLUTIONS.items():\n",
    "        has_data = (res_folder in y_true_by_res and \n",
    "                    res_folder in y_probs_by_res_model and \n",
    "                    model_folder in y_probs_by_res_model.get(res_folder, {}))\n",
    "        \n",
    "        if not has_data:\n",
    "            continue\n",
    "        \n",
    "        y_true = y_true_by_res[res_folder]\n",
    "        y_probs = y_probs_by_res_model[res_folder][model_folder]\n",
    "        brier = brier_score_loss(y_true, y_probs)\n",
    "        color = colors.get(res_folder, \"blue\")\n",
    "        \n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_probs,\n",
    "            n_bins=10,\n",
    "            strategy=\"uniform\",\n",
    "            ax=ax,\n",
    "            color=color,\n",
    "            name=f\"{res_display} (Brier: {brier:.4f})\",\n",
    "            linewidth=2.5,\n",
    "            marker='o',\n",
    "            markersize=6\n",
    "        )\n",
    "    \n",
    "    # Format each subplot\n",
    "    pub_name = pub_model_names.get(model_folder, model_display)\n",
    "    ax.set_title(f\"{pub_name}\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "    ax.set_xlabel(\"Mean Predicted Probability (AFib)\", fontsize=11)\n",
    "    ax.set_ylabel(\"Fraction of Positives (AFib)\", fontsize=11)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(0.02, 0.98), \n",
    "             fontsize=9, framealpha=0.95, edgecolor='gray')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=2, alpha=0.5, zorder=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save combined figure\n",
    "combined_filename = \"fig_calibration.png\"\n",
    "plt.savefig(combined_filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"  âœ… Saved combined figure: {combined_filename}\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"âœ… PUBLICATION-READY PLOTS GENERATED\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# LaTeX-ready summary\n",
    "print(\"\\nğŸ“„ LaTeX-ready filenames for \\\\includegraphics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"fig_calibration.png          % figure* (both models side-by-side)\")\n",
    "for fname in overlay_files:\n",
    "    model_type = fname.split('_')[1].replace('.png', '')\n",
    "    print(f\"{fname:<28} % Individual {model_type.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“Š Files generated:\")\n",
    "print(f\"  â€¢ Combined figure: {combined_filename}\")\n",
    "print(f\"  â€¢ Individual plots: {', '.join(overlay_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fba08",
   "metadata": {},
   "source": [
    "## Summary: Saved PNG Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432457a1",
   "metadata": {},
   "source": [
    "## ğŸ“Š Expected Output\n",
    "\n",
    "**Individual Calibration Curve PNG files** (8 files - 1 per model-resolution combo):\n",
    "\n",
    "1. calibration_1DCNN_625Hz.png\n",
    "2. calibration_1DCNN_125Hz.png\n",
    "3. calibration_1DCNN_250Hz.png\n",
    "4. calibration_1DCNN_500Hz.png\n",
    "5. calibration_HybridCNNLSTM_625Hz.png\n",
    "6. calibration_HybridCNNLSTM_125Hz.png\n",
    "7. calibration_HybridCNNLSTM_250Hz.png\n",
    "8. calibration_HybridCNNLSTM_500Hz.png\n",
    "\n",
    "**2Ã—2 Comparison Grid PNG files** (2 files - 1 per model with 2Ã—2 grid):\n",
    "\n",
    "9. calibration_comparison_1D-CNN.png (all 4 resolutions in 2Ã—2 grid)\n",
    "10. calibration_comparison_HybridCNNLSTM.png (all 4 resolutions in 2Ã—2 grid)\n",
    "\n",
    "**Overlay Comparison PNG files** (2 files - 1 per model with all resolutions overlaid):\n",
    "\n",
    "11. calibration_overlay_1D-CNN.png (all 4 resolutions with distinct colors)\n",
    "12. calibration_overlay_HybridCNNLSTM.png (all 4 resolutions with distinct colors)\n",
    "\n",
    "### File Descriptions\n",
    "\n",
    "**Individual PNG** (8 files):\n",
    "- **Content**: Single calibration curve per model-resolution combination\n",
    "- **Colors**: Blue curve with diagonal reference\n",
    "- **Size**: 10Ã—5 inches, 300 dpi\n",
    "- **Use**: Detailed view of each model-resolution pair\n",
    "\n",
    "**2Ã—2 Grid PNG** (2 files):\n",
    "- **Content**: All 4 resolutions for one model in a 2Ã—2 grid layout\n",
    "- **Consistent scales**: Easy visual comparison\n",
    "- **Colors**: Blue curves with consistent scaling\n",
    "- **Size**: 14Ã—12 inches, 300 dpi\n",
    "- **Use**: Comparing how a single model performs across different resolutions\n",
    "\n",
    "**Overlay PNG** (2 files):\n",
    "- **Content**: All 4 resolutions for one model overlaid on the same plot\n",
    "- **Color gradient**: \n",
    "  - 62.5Hz: Bright Yellow (#FFFF00)\n",
    "  - 125Hz: Orange (#FFA500)\n",
    "  - 250Hz: Tomato Red (#FF6347)\n",
    "  - 500Hz: Dark Red (#8B0000)\n",
    "- **Size**: 12Ã—8 inches, 300 dpi\n",
    "- **Use**: Direct visual comparison of resolution impact on calibration, following each resolution's colored line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e68a5a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated individual PNG files:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. calibration_1D-CNN_625Hz.png\n",
      " 2. calibration_Hybrid_CNNLSTM_625Hz.png\n",
      " 3. calibration_1D-CNN_125Hz.png\n",
      " 4. calibration_Hybrid_CNNLSTM_125Hz.png\n",
      " 5. calibration_1D-CNN_250Hz.png\n",
      " 6. calibration_Hybrid_CNNLSTM_250Hz.png\n",
      " 7. calibration_1D-CNN_500Hz.png\n",
      " 8. calibration_Hybrid_CNNLSTM_500Hz.png\n",
      "\n",
      "================================================================================\n",
      "Total individual files saved: 8\n",
      "================================================================================\n",
      "\n",
      "Generated comparison PNG files:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. calibration_comparison_1D-CNN.png\n",
      " 2. calibration_comparison_Hybrid_CNNLSTM.png\n",
      "\n",
      "================================================================================\n",
      "Total comparison files saved: 2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated individual PNG files:\")\n",
    "print(\"-\" * 80)\n",
    "for i, fname in enumerate(saved_files, 1):\n",
    "    print(f\"{i:2d}. {fname}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total individual files saved: {len(saved_files)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'comparison_files' in dir() and len(comparison_files) > 0:\n",
    "    print(\"\\nGenerated comparison PNG files:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, fname in enumerate(comparison_files, 1):\n",
    "        print(f\"{i:2d}. {fname}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total comparison files saved: {len(comparison_files)}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898cfc2",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Metrics Guide\n",
    "\n",
    "### Brier Score (Calibration Error)\n",
    "- **Range**: 0 (perfect) to 1 (worst)\n",
    "- **Typical clinical models**: 0.1â€“0.3\n",
    "- **Interpretation**: Mean squared error between predicted probability and true label\n",
    "\n",
    "### Calibration Curve Positioning\n",
    "- **Curve ABOVE diagonal**: Model is **under-confident** (predicted prob < true frequency)\n",
    "- **Curve BELOW diagonal**: Model is **over-confident** (predicted prob > true frequency)  \n",
    "- **Curve ON diagonal**: **Perfect calibration** (ideal)\n",
    "\n",
    "### AFib Detection Relevance\n",
    "- Under-confident: Risk of missing AFib cases (safety concern)\n",
    "- Over-confident: Unnecessary referrals (cost concern)\n",
    "- Well-calibrated: Clinicians can trust probability scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413f4cb",
   "metadata": {},
   "source": [
    "## Bonus: 2Ã—2 Subplots â€“ All Resolutions Per Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0370b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_subplots_2x2(model_name, y_true_by_res, predictions_by_res):\n",
    "    \"\"\"\n",
    "    Plot 2Ã—2 grid of calibration curves for all 4 resolutions of a single model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: str, \"1D-CNN\" or \"Hybrid CNN+LSTM\"\n",
    "        y_true_by_res: dict {resolution: y_true_array}\n",
    "        predictions_by_res: dict {resolution: {model_name: y_probs_array}}\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, resolution in enumerate(RESOLUTIONS):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if resolution not in y_true_by_res or model_name not in predictions_by_res.get(resolution, {}):\n",
    "            ax.text(0.5, 0.5, f\"No data for {resolution}Hz\", ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "        \n",
    "        y_true = y_true_by_res[resolution]\n",
    "        y_probs = predictions_by_res[resolution][model_name]\n",
    "        \n",
    "        # Compute metrics\n",
    "        brier = brier_score_loss(y_true, y_probs)\n",
    "        mean_pred = np.mean(y_probs)\n",
    "        mean_true = np.mean(y_true)\n",
    "        \n",
    "        # Generate calibration curve on this subplot\n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_probs,\n",
    "            n_bins=10,\n",
    "            strategy=\"uniform\",\n",
    "            ax=ax,\n",
    "            color=\"steelblue\"\n",
    "        )\n",
    "        \n",
    "        # Add perfect calibration diagonal\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Title and labels\n",
    "        ax.set_title(f\"{resolution}Hz (n={len(y_true)})\", fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Mean Predicted Probability\", fontsize=10)\n",
    "        ax.set_ylabel(\"True Positive Fraction\", fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add Brier score\n",
    "        ax.text(0.98, 0.02, f\"Brier: {brier:.4f}\", transform=ax.transAxes,\n",
    "                fontsize=9, verticalalignment='bottom', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    fig.suptitle(f\"Calibration Curves â€“ {model_name} (All Resolutions)\", \n",
    "                 fontsize=16, fontweight=\"bold\", y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_filename = f\"calibration_subplots_{model_name.replace('+', '_').replace(' ', '')}.png\"\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: {output_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example (run after main loop):\n",
    "# Collect data per resolution if needed\n",
    "# y_true_by_res = {res: ... for res in RESOLUTIONS}\n",
    "# predictions_by_res = {res: {model: ... for model in MODELS} for res in RESOLUTIONS}\n",
    "# for model in MODELS:\n",
    "#     plot_model_subplots_2x2(model, y_true_by_res, predictions_by_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99617249",
   "metadata": {},
   "source": [
    "## Summary Table: Calibration Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00abb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "CALIBRATION METRICS SUMMARY TABLE\n",
      "========================================================================================================================\n",
      "          Model Resolution  Brier Score  N Samples  Mean Pred Prob  Mean True Label\n",
      "         1D-CNN     62.5Hz     0.010784       2185        0.142910         0.133181\n",
      "Hybrid CNN+LSTM     62.5Hz     0.015501       2185        0.144664         0.133181\n",
      "         1D-CNN      125Hz     0.013038       2185        0.145575         0.133181\n",
      "Hybrid CNN+LSTM      125Hz     0.012609       2185        0.144818         0.133181\n",
      "         1D-CNN      250Hz     0.013949       2185        0.145526         0.133181\n",
      "Hybrid CNN+LSTM      250Hz     0.016043       2185        0.144139         0.133181\n",
      "         1D-CNN      500Hz     0.028855       2185        0.164796         0.133181\n",
      "Hybrid CNN+LSTM      500Hz     0.015197       2185        0.152052         0.133181\n",
      "\n",
      "========================================================================================================================\n",
      "INTERPRETATION GUIDE\n",
      "========================================================================================================================\n",
      "\n",
      "1. BRIER SCORE (lower is better):\n",
      "   â€¢ Mean squared error between predicted probability and true label\n",
      "   â€¢ Range: 0 (perfect) to 1 (worst)\n",
      "   â€¢ Typical clinical models: 0.1â€“0.3\n",
      "   â€¢ Interpretation: Measures calibration error and classification accuracy combined\n",
      "\n",
      "2. MEAN PREDICTED PROB (AFib):\n",
      "   â€¢ Average confidence the model assigns to AFib (class=1)\n",
      "   â€¢ If much lower than \"Mean True Label\": Model is UNDERCONFIDENT\n",
      "   â€¢ If much higher than \"Mean True Label\": Model is OVERCONFIDENT\n",
      "   â€¢ Ideal: Close to \"Mean True Label\" value\n",
      "\n",
      "3. MEAN TRUE LABEL (AFib):\n",
      "   â€¢ True prevalence of AFib in test set\n",
      "   â€¢ Indicates class balance (0.5 = balanced, <0.5 = AFib underrepresented)\n",
      "\n",
      "4. CALIBRATION CURVE POSITIONING:\n",
      "   â€¢ Above diagonal: Predicted probability < actual outcome frequency (underconfident)\n",
      "   â€¢ Below diagonal: Predicted probability > actual outcome frequency (overconfident)\n",
      "   â€¢ On diagonal: Perfect calibration (ideal)\n",
      "\n",
      "CLINICAL RELEVANCE FOR AFib DETECTION:\n",
      "   â€¢ A well-calibrated model enables clinicians to interpret confidence scores meaningfully\n",
      "   â€¢ E.g., if model predicts 0.8 AFib probability, it should reflect ~80% actual AFib likelihood\n",
      "   â€¢ Underconfidence â†’ risk of missing AFib cases (safety concern)\n",
      "   â€¢ Overconfidence â†’ unnecessary referrals/treatments (cost/burden concern)\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "AGGREGATE STATISTICS\n",
      "========================================================================================================================\n",
      "Average Brier Score:        0.015747\n",
      "  Min: 0.010784 (1D-CNN @ 62.5Hz)\n",
      "  Max: 0.028855 (1D-CNN @ 500Hz)\n",
      "\n",
      "Avg calibration gap (|pred - true|): 0.0149\n",
      "  Min gap: 0.0097\n",
      "  Max gap: 0.0316\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create and display summary table\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"CALIBRATION METRICS SUMMARY TABLE\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if len(results_list) == 0:\n",
    "    print(\"âš  No results generated. Check that prediction files exist.\")\n",
    "else:\n",
    "    # Create DataFrame from results\n",
    "    df_summary = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Select and reorder columns for display\n",
    "    display_cols = [\"Model\", \"Resolution\", \"Brier Score\", \"N Samples\", \"Mean Pred Prob\", \"Mean True Label\"]\n",
    "    df_display = df_summary[display_cols].copy()\n",
    "    \n",
    "    # Sort by resolution then model\n",
    "    res_order = [\"62.5Hz\", \"125Hz\", \"250Hz\", \"500Hz\"]\n",
    "    df_display['Resolution'] = pd.Categorical(df_display['Resolution'], categories=res_order, ordered=True)\n",
    "    df_display = df_display.sort_values(by=['Resolution', 'Model']).reset_index(drop=True)\n",
    "    \n",
    "    # Format for display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"INTERPRETATION GUIDE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(\"\"\"\n",
    "1. BRIER SCORE (lower is better):\n",
    "   â€¢ Mean squared error between predicted probability and true label\n",
    "   â€¢ Range: 0 (perfect) to 1 (worst)\n",
    "   â€¢ Typical clinical models: 0.1â€“0.3\n",
    "   â€¢ Interpretation: Measures calibration error and classification accuracy combined\n",
    "\n",
    "2. MEAN PREDICTED PROB (AFib):\n",
    "   â€¢ Average confidence the model assigns to AFib (class=1)\n",
    "   â€¢ If much lower than \"Mean True Label\": Model is UNDERCONFIDENT\n",
    "   â€¢ If much higher than \"Mean True Label\": Model is OVERCONFIDENT\n",
    "   â€¢ Ideal: Close to \"Mean True Label\" value\n",
    "\n",
    "3. MEAN TRUE LABEL (AFib):\n",
    "   â€¢ True prevalence of AFib in test set\n",
    "   â€¢ Indicates class balance (0.5 = balanced, <0.5 = AFib underrepresented)\n",
    "\n",
    "4. CALIBRATION CURVE POSITIONING:\n",
    "   â€¢ Above diagonal: Predicted probability < actual outcome frequency (underconfident)\n",
    "   â€¢ Below diagonal: Predicted probability > actual outcome frequency (overconfident)\n",
    "   â€¢ On diagonal: Perfect calibration (ideal)\n",
    "\n",
    "CLINICAL RELEVANCE FOR AFib DETECTION:\n",
    "   â€¢ A well-calibrated model enables clinicians to interpret confidence scores meaningfully\n",
    "   â€¢ E.g., if model predicts 0.8 AFib probability, it should reflect ~80% actual AFib likelihood\n",
    "   â€¢ Underconfidence â†’ risk of missing AFib cases (safety concern)\n",
    "   â€¢ Overconfidence â†’ unnecessary referrals/treatments (cost/burden concern)\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"AGGREGATE STATISTICS\")\n",
    "    print(\"=\" * 120)\n",
    "    brier_scores = df_display['Brier Score'].values\n",
    "    print(f\"Average Brier Score:        {np.mean(brier_scores):.6f}\")\n",
    "    print(f\"  Min: {np.min(brier_scores):.6f} ({df_display.iloc[np.argmin(brier_scores)]['Model']} @ {df_display.iloc[np.argmin(brier_scores)]['Resolution']})\")\n",
    "    print(f\"  Max: {np.max(brier_scores):.6f} ({df_display.iloc[np.argmax(brier_scores)]['Model']} @ {df_display.iloc[np.argmax(brier_scores)]['Resolution']})\")\n",
    "    \n",
    "    mean_preds = df_display['Mean Pred Prob'].values\n",
    "    mean_trues = df_display['Mean True Label'].values\n",
    "    calibration_gaps = np.abs(mean_preds - mean_trues)\n",
    "    print(f\"\\nAvg calibration gap (|pred - true|): {np.mean(calibration_gaps):.4f}\")\n",
    "    print(f\"  Min gap: {np.min(calibration_gaps):.4f}\")\n",
    "    print(f\"  Max gap: {np.max(calibration_gaps):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
