{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file existence\n",
    "existing_files = cleaned_mapping['ecg_path'].apply(lambda p: Path(p).exists())\n",
    "existence_rate = existing_files.sum() / len(cleaned_mapping)\n",
    "\n",
    "print(f\"File Existence Check:\")\n",
    "print(f\"  Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"  Files exist: {existing_files.sum():,}\")\n",
    "print(f\"  Files missing: {(~existing_files).sum():,}\")\n",
    "print(f\"  Existence rate: {existence_rate:.2%}\")\n",
    "\n",
    "# Assert at least 95% exist\n",
    "assert existence_rate >= 0.95, f\"Only {existence_rate:.2%} of files exist (expected ≥95%)\"\n",
    "print(f\"  ✓ Assertion passed: ≥95% of files exist\")\n",
    "\n",
    "# Label distribution\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(f\"  AFIB records: {cleaned_mapping['_AFIB'].sum():,}\")\n",
    "print(f\"  SR records:   {cleaned_mapping['_SR'].sum():,}\")\n",
    "\n",
    "# Sample rows\n",
    "print(f\"\\nSample rows from cleaned_mapping:\")\n",
    "print(cleaned_mapping.sample(n=min(10, len(cleaned_mapping)), random_state=42))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Data handling complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5881dd",
   "metadata": {},
   "source": [
    "## 7. Sanity Checks\n",
    "\n",
    "Verify that at least 95% of the ecg_path files exist, and display sample statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c29a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final cleaned mapping with required columns\n",
    "cleaned_mapping = balanced_df[['record_id', 'ecg_path', '_AFIB', '_SR']].copy()\n",
    "\n",
    "# Convert Path objects to strings for CSV serialization\n",
    "cleaned_mapping['ecg_path'] = cleaned_mapping['ecg_path'].apply(str)\n",
    "\n",
    "# Save to CSV\n",
    "cleaned_mapping.to_csv(OUT_MAPPING_CSV, index=False)\n",
    "\n",
    "print(f\"✓ Saved cleaned mapping to: {OUT_MAPPING_CSV}\")\n",
    "print(f\"  Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"\\nFirst 5 rows of cleaned_mapping:\")\n",
    "cleaned_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d98a80",
   "metadata": {},
   "source": [
    "## 6. Create Final Mapping & Save\n",
    "\n",
    "Select only the required columns and save to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2889afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all records: construct paths and copy files\n",
    "ecg_paths = []\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(\"Processing records and copying files...\\n\")\n",
    "\n",
    "for idx, row in balanced_df.iterrows():\n",
    "    record_id = row['record_id']\n",
    "    \n",
    "    # Construct output path\n",
    "    output_path = construct_output_path(record_id, CLEANED_WFDB_DIR)\n",
    "    ecg_paths.append(output_path)\n",
    "    \n",
    "    # Copy or generate the .npy file\n",
    "    success = copy_or_generate_npy(record_id, RAW_WFDB_DIR, output_path)\n",
    "    \n",
    "    if success:\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        failed_count += 1\n",
    "    \n",
    "    # Progress update every 500 records\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"Processed {idx + 1:,}/{len(balanced_df):,} records (copied: {copied_count}, failed: {failed_count})\")\n",
    "\n",
    "print(f\"\\n✓ Processing complete\")\n",
    "print(f\"  Successfully copied/generated: {copied_count:,} files\")\n",
    "print(f\"  Failed: {failed_count:,} files\")\n",
    "\n",
    "# Add ecg_path column to the DataFrame\n",
    "balanced_df['ecg_path'] = ecg_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_output_path(record_id: str, cleaned_wfdb_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Construct output path using bucket structure:\n",
    "    cleaned_wfdb_dir/12/120/120.npy for record_id='120'\n",
    "    \"\"\"\n",
    "    # Extract first 2 characters as bucket\n",
    "    bucket = record_id[:2]\n",
    "    \n",
    "    # Build path: bucket/record_id/record_id.npy\n",
    "    output_path = cleaned_wfdb_dir / bucket / record_id / f\"{record_id}.npy\"\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def copy_or_generate_npy(\n",
    "    record_id: str,\n",
    "    raw_wfdb_dir: Path,\n",
    "    output_path: Path\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Copy or generate .npy file from raw WFDB data.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if already exists\n",
    "    if output_path.exists():\n",
    "        return True\n",
    "    \n",
    "    # Look for raw .npy file first (if already converted)\n",
    "    # The raw CSV path is structured like: 01/010/JS00001.csv\n",
    "    # We need to find the corresponding .npy or read from WFDB format\n",
    "    \n",
    "    # Try to find the source file in RAW_WFDB_DIR\n",
    "    # The record_id starts with 'JS' typically, so we search for it\n",
    "    possible_sources = list(raw_wfdb_dir.rglob(f\"*{record_id}.*\"))\n",
    "    \n",
    "    # Filter for .npy files first\n",
    "    npy_sources = [p for p in possible_sources if p.suffix == '.npy']\n",
    "    if npy_sources:\n",
    "        shutil.copy2(npy_sources[0], output_path)\n",
    "        return True\n",
    "    \n",
    "    # If no .npy, try to read WFDB format (.hea, .dat files)\n",
    "    hea_sources = [p for p in possible_sources if p.suffix == '.hea']\n",
    "    if hea_sources:\n",
    "        try:\n",
    "            # Read WFDB record (without extension)\n",
    "            record_path = str(hea_sources[0].parent / hea_sources[0].stem)\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            \n",
    "            # Convert to numpy and save\n",
    "            ecg_data = record.p_signal  # Shape: (n_samples, n_leads)\n",
    "            np.save(output_path, ecg_data)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to read WFDB for {record_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # If CSV exists, try reading it\n",
    "    csv_sources = [p for p in possible_sources if p.suffix == '.csv']\n",
    "    if csv_sources:\n",
    "        try:\n",
    "            csv_data = pd.read_csv(csv_sources[0])\n",
    "            ecg_array = csv_data.values\n",
    "            np.save(output_path, ecg_array)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to read CSV for {record_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"  Warning: No source file found for {record_id}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81ed2a",
   "metadata": {},
   "source": [
    "## 5. Path Construction & File Copying\n",
    "\n",
    "For each record, construct the output path using the bucket structure:\n",
    "- Bucket = first 2 digits of `record_id`\n",
    "- Path = `CLEANED_WFDB_DIR / bucket / record_id / <record_id>.npy`\n",
    "\n",
    "We'll copy/generate .npy files from the raw WFDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary label columns with underscore prefix\n",
    "mapping_df['_AFIB'] = mapping_df['AFIB'].astype(int)\n",
    "mapping_df['_SR'] = mapping_df['SR'].astype(int)\n",
    "\n",
    "# Filter to records with exactly one of AFIB or SR (no multi-label, no unlabeled)\n",
    "valid_records = mapping_df[\n",
    "    ((mapping_df['_AFIB'] == 1) & (mapping_df['_SR'] == 0)) |\n",
    "    ((mapping_df['_AFIB'] == 0) & (mapping_df['_SR'] == 1))\n",
    "].copy()\n",
    "\n",
    "print(f\"Original dataset: {len(mapping_df):,} records\")\n",
    "print(f\"Valid AFIB-only or SR-only records: {len(valid_records):,} records\")\n",
    "\n",
    "# Count AFIB vs SR before balancing\n",
    "afib_records = valid_records[valid_records['_AFIB'] == 1]\n",
    "sr_records = valid_records[valid_records['_SR'] == 1]\n",
    "\n",
    "print(f\"\\n--- Before Balancing ---\")\n",
    "print(f\"AFIB records: {len(afib_records):,}\")\n",
    "print(f\"SR records:   {len(sr_records):,}\")\n",
    "\n",
    "# Balance by downsampling the majority class\n",
    "min_count = min(len(afib_records), len(sr_records))\n",
    "afib_balanced = afib_records.sample(n=min_count, random_state=42)\n",
    "sr_balanced = sr_records.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat([afib_balanced, sr_balanced], ignore_index=True)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- After Balancing ---\")\n",
    "print(f\"AFIB records: {len(afib_balanced):,}\")\n",
    "print(f\"SR records:   {len(sr_balanced):,}\")\n",
    "print(f\"Total balanced records: {len(balanced_df):,}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset balanced successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc7789",
   "metadata": {},
   "source": [
    "## 4. Label Engineering & Balancing\n",
    "\n",
    "Create binary columns `_AFIB` and `_SR`, then balance the dataset by downsampling the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original mapping CSV\n",
    "mapping_df = pd.read_csv(RAW_MAPPING_CSV)\n",
    "\n",
    "print(f\"Loaded {len(mapping_df):,} records from {RAW_MAPPING_CSV.name}\")\n",
    "print(f\"\\nColumns available: {list(mapping_df.columns)[:10]}... (showing first 10)\")\n",
    "print(f\"\\nDataFrame shape: {mapping_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "mapping_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed194a9",
   "metadata": {},
   "source": [
    "## 3. Load Original File Mapping\n",
    "\n",
    "We expect the raw mapping CSV at: `data/ecg_arrhythmia_dataset_CSV/file_mapping.csv`\n",
    "\n",
    "This CSV contains:\n",
    "- `record_id`: Unique identifier (e.g., JS00001)\n",
    "- `AFIB`: Binary column (1 = atrial fibrillation present)\n",
    "- `SR`: Binary column (1 = sinus rhythm present)\n",
    "- Other diagnostic and demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure paths (relative, portable)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Raw dataset (input)\n",
    "RAW_WFDB_DIR = DATA_DIR / \"ecg_arrhythmia_dataset_CSV\" / \"WFDBRecords\"\n",
    "RAW_MAPPING_CSV = DATA_DIR / \"ecg_arrhythmia_dataset_CSV\" / \"file_mapping.csv\"\n",
    "\n",
    "# Cleaned dataset (output)\n",
    "CLEANED_ROOT = DATA_DIR / \"cleaned_balanced_AFIB_SR\"\n",
    "CLEANED_WFDB_DIR = CLEANED_ROOT / \"WFDBRecords\"\n",
    "OUT_MAPPING_CSV = CLEANED_ROOT / \"file_mapping_cleaned.csv\"\n",
    "\n",
    "# Create output directories\n",
    "CLEANED_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "CLEANED_WFDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"RAW_MAPPING_CSV: {RAW_MAPPING_CSV}\")\n",
    "print(f\"RAW_WFDB_DIR: {RAW_WFDB_DIR}\")\n",
    "print(f\"OUT_MAPPING_CSV: {OUT_MAPPING_CSV}\")\n",
    "print(f\"CLEANED_WFDB_DIR: {CLEANED_WFDB_DIR}\")\n",
    "print(f\"\\n✓ Paths configured (all relative to project root)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b259a9",
   "metadata": {},
   "source": [
    "## 2. Path Setup\n",
    "\n",
    "Define all paths relative to the project root—no hardcoded C:\\ paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "import wfdb\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0697b",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2ce6f",
   "metadata": {},
   "source": [
    "# ECG Data Handling: Cleaned & Balanced AFIB/SR Dataset\n",
    "\n",
    "This notebook creates a cleaned and balanced subset of AFIB vs SR records from the original ECG arrhythmia database.\n",
    "\n",
    "**Steps:**\n",
    "1. Define project-relative paths\n",
    "2. Load original file mapping\n",
    "3. Engineer binary labels (_AFIB, _SR) and balance the dataset\n",
    "4. Construct output paths and copy/generate .npy files\n",
    "5. Save cleaned mapping CSV with sanity checks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
